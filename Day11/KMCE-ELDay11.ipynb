{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM (**Long Short-Term Memory**) solves the **vanishing gradient problem** of **RNNs** by using **gates** and a **cell state**. Let's break it down **mathematically and conceptually**, ensuring **clarity and precision**.\n",
    "# **1. Core Concept of LSTM**\n",
    "- Unlike a vanilla **RNN**, an **LSTM** maintains a **cell state** $ C_t $, which carries **long-term dependencies**.\n",
    "- Information flow is regulated by **three gates**:\n",
    "  1. **Forget Gate** ‚Äì Decides what information to discard from the past.\n",
    "  2. **Input Gate** ‚Äì Decides what new information to store.\n",
    "  3. **Output Gate** ‚Äì Decides what to output at the current time step.\n",
    "\n",
    "# **2. LSTM Cell Structure**\n",
    "Each **LSTM unit** takes:\n",
    "- **Input**: $ x_t $ (current input at time step $$ t $$).\n",
    "- **Previous Hidden State**: $ h_{t-1} $ (from the last time step).\n",
    "- **Previous Cell State**: $$ C_{t-1} $$ (stores long-term memory).\n",
    "\n",
    "It updates:\n",
    "- **New Cell State**: $$ C_t $$.\n",
    "- **New Hidden State**: $$ h_t $$ (which serves as output).\n",
    "\n",
    "# **3. LSTM Equations (Mathematical Formulation)**  \n",
    "Each gate uses a **sigmoid activation function** $$ \\sigma $$ to control information flow.\n",
    "\n",
    "## **(a) Forget Gate**\n",
    "Decides **what to forget** from the past:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n",
    "$$\n",
    "\n",
    "- $$ f_t $$ ‚Üí Forget gate activation (values between **0** and **1**).\n",
    "- $$ W_f, U_f, b_f $$ ‚Üí Weight matrices and bias for forget gate.\n",
    "\n",
    "üîπ If $$ f_t \\approx 1 $$, past information is **retained**.  \n",
    "üîπ If $$ f_t \\approx 0 $$, past information is **discarded**.\n",
    "\n",
    "### **Cell State Update (Applying Forget Gate)**\n",
    "$$\n",
    "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "$$\n",
    "This ensures old memory is **partially forgotten** while new memory is **added**.\n",
    "\n",
    "## **(b) Input Gate**\n",
    "Decides **what new information to store** in the cell state:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n",
    "$$\n",
    "\n",
    "- $$ i_t $$ ‚Üí Input gate activation (**controls new information storage**).\n",
    "- $$ W_i, U_i, b_i $$ ‚Üí Weight matrices and bias.\n",
    "\n",
    "A candidate memory update is created:\n",
    "\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C x_t + U_C h_{t-1} + b_C)\n",
    "$$\n",
    "\n",
    "- $$ \\tilde{C}_t $$ ‚Üí Candidate cell state (potential new memory).\n",
    "- Uses **tanh** to keep values between **‚àí1 and 1**.\n",
    "\n",
    "### **Cell State Update (Applying Input Gate)**\n",
    "$$\n",
    "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "- $$ f_t C_{t-1} $$ ‚Üí Keeps part of **old memory**.\n",
    "- $$ i_t \\tilde{C}_t $$ ‚Üí Adds **new information**.\n",
    "\n",
    "## **(c) Output Gate**\n",
    "Decides **what to output** as the hidden state:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n",
    "$$\n",
    "\n",
    "- $$ o_t $$ ‚Üí Output gate activation (**controls what is sent to the next layer**).\n",
    "- $$ W_o, U_o, b_o $$ ‚Üí Weight matrices and bias.\n",
    "\n",
    "Final hidden state:\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "- $$ h_t $$ ‚Üí Output (hidden state).\n",
    "- Uses **tanh** to scale $$ C_t $$ values.\n",
    "\n",
    "# **4. Summary of LSTM Computation**\n",
    "| Gate | Equation | Function |\n",
    "|------|---------|----------|\n",
    "| **Forget Gate** | $$ f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) $$ | Decides what to forget |\n",
    "| **Input Gate** | $$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) $$ | Decides what new info to store |\n",
    "| **Candidate Memory** | $$ \\tilde{C}_t = \\tanh(W_C x_t + U_C h_{t-1} + b_C) $$ | Generates possible new memory |\n",
    "| **Cell State Update** | $$ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t $$ | Updates long-term memory |\n",
    "| **Output Gate** | $$ o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) $$ | Decides what to output |\n",
    "| **Hidden State** | $$ h_t = o_t \\odot \\tanh(C_t) $$ | Final output |\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Key Benefits of LSTM**\n",
    "‚úÖ **Solves vanishing gradient** (by direct connections in cell state).  \n",
    "‚úÖ **Captures long-term dependencies** (uses **cell state** instead of overwriting memory).  \n",
    "‚úÖ **Gates regulate information flow**, preventing **unnecessary updates**.  \n",
    "\n",
    "# **6. Limitations of LSTM**\n",
    "‚ùå **Computationally expensive** (more parameters than RNN).  \n",
    "‚ùå **Cannot capture very long dependencies** in extremely long sequences.  \n",
    "‚ùå **Still sequential** (cannot parallelize well like Transformers).  \n",
    "\n",
    "To overcome this, modern architectures use **GRU (simpler than LSTM)** or **Transformers (self-attention mechanism)**.\n",
    "\n",
    "# **Conclusion**\n",
    "- **LSTM uses gates to control memory storage, forgetting, and output.**\n",
    "- **Cell state acts as a highway to carry long-term dependencies.**\n",
    "- **It improves over RNNs by solving vanishing gradients but is computationally expensive.**\n",
    "- **Transformers (e.g., GPT, BERT) are now preferred over LSTMs for NLP tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Limitations of RNNs**\n",
    "## **1.1. Exploding and Vanishing Gradient Problems**\n",
    "When training an **RNN**, we use **Backpropagation Through Time (BPTT)** to update weights. However, during training, gradients can either **explode** or **vanish**, making learning unstable or ineffective.\n",
    "\n",
    "### **(a) Exploding Gradient Problem**\n",
    "- **What happens?**  \n",
    "  - Gradients grow **too large** during backpropagation.  \n",
    "  - Leads to **unstable updates**, where **weights oscillate wildly** and fail to converge.  \n",
    "  - **Symptoms**: Loss becomes `NaN`, model performance fluctuates, no meaningful learning.  \n",
    "- **Why does it happen?**  \n",
    "  - When the weight matrix **W** has large eigenvalues, backpropagating through time **amplifies** gradients exponentially.  \n",
    "- **Solution?**\n",
    "  - **Gradient Clipping**: Limits the gradient magnitude to prevent instability.\n",
    "\n",
    "### **(b) Vanishing Gradient Problem**\n",
    "- **What happens?**  \n",
    "  - Gradients **shrink too much**, leading to **very small weight updates**.  \n",
    "  - The network **stops learning long-term dependencies** because older time steps **lose influence**.  \n",
    "  - **Symptoms**: Model ignores long-term patterns, only learns short-term dependencies.  \n",
    "- **Why does it happen?**  \n",
    "  - If weight values in **W** are **small**, multiplying them repeatedly **shrinks** gradients exponentially, leading to near-zero updates.  \n",
    "- **Solution?**\n",
    "  - **Using LSTMs or GRUs**, which preserve information over long sequences.\n",
    "\n",
    "## **1.2. Long-Term Dependency Problem**\n",
    "- **What happens?**  \n",
    "  - Standard RNNs **struggle** to remember information from **far-back time steps**.  \n",
    "  - Works **well for short-term sequences**, but **fails when long-term memory is needed**.  \n",
    "  - Example: In a sentence like *‚ÄúThe clouds are in the sky. The sun is shining. It is a beautiful day.‚Äù*, a regular RNN might forget *\"The clouds are in the sky‚Äù* when predicting *‚ÄúIt is a beautiful day‚Äù*.  \n",
    "- **Why does it happen?**  \n",
    "  - Due to **vanishing gradients**, older inputs don‚Äôt significantly contribute to weight updates.  \n",
    "- **Solution?**\n",
    "  - **LSTMs and GRUs** solve this using **gates**, which selectively remember important information.\n",
    "\n",
    "# **2. How Does LSTM Solve These Issues?**\n",
    "## **2.1. What is an LSTM?**\n",
    "LSTM (Long Short-Term Memory) is a special type of **RNN** that introduces **gates** to **control** what information is kept or forgotten over time.\n",
    "\n",
    "### **(a) Memory Cell**\n",
    "- Unlike RNNs, LSTMs have an **explicit memory cell** that can **retain information over long sequences**.  \n",
    "- This prevents vanishing gradients and enables **long-term dependencies** to be captured.\n",
    "\n",
    "### **(b) Gates in LSTM**\n",
    "LSTMs use **three gates** to control information flow:\n",
    "\n",
    "#### **1Ô∏è‚É£ Forget Gate $$ f_t $$**\n",
    "$$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "- **Decides what to forget** from the past.\n",
    "- If $$ f_t $$ is **close to 0**, it forgets that piece of information.  \n",
    "- If $$ f_t $$ is **close to 1**, it retains it.  \n",
    "\n",
    "#### **2Ô∏è‚É£ Input Gate $$ i_t $$**\n",
    "$$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "- **Decides what new information to store.**\n",
    "- ** $$ i_t  $$** determines how much new information is added to memory.  \n",
    "- $$ C_t $$ is the **candidate memory update**.  \n",
    "\n",
    "#### **3Ô∏è‚É£ Output Gate $$ o_t $$**\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(C_t)\n",
    "$$\n",
    "- **Controls what part of the memory is output.**  \n",
    "- The hidden state $$ h_t $$ is **filtered** by $$ o_t $$, deciding what to pass to the next step.  \n",
    "\n",
    "### **(c) Memory Cell Update**\n",
    "$$\n",
    "C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "$$\n",
    "- The memory cell **accumulates useful information** while discarding unnecessary data.  \n",
    "- **Prevents vanishing gradients** by allowing a direct flow of information.\n",
    "\n",
    "# **3. Limitations of RNNs and LSTMs**\n",
    "## **3.1. Limitations of RNNs**\n",
    "| Problem | Cause | Solution |\n",
    "|---------|-------|----------|\n",
    "| **Exploding Gradients** | Large weight updates | Gradient Clipping |\n",
    "| **Vanishing Gradients** | Small weight updates | LSTM / GRU |\n",
    "| **Short-term Memory** | Loss of long-term dependencies | LSTM |\n",
    "| **Slow Training** | Sequential processing | Parallelization in Transformers |\n",
    "\n",
    "## **3.2. Limitations of LSTMs**\n",
    "| Limitation | Why? | Solution |\n",
    "|------------|------|----------|\n",
    "| **Slow Computation** | Too many parameters | GRU (fewer parameters) |\n",
    "| **Cannot Handle Very Long Sequences** | Still sequential | Transformers |\n",
    "| **Hard to Parallelize** | Each step depends on the previous step | Self-Attention |\n",
    "\n",
    "# **4. Why Transformers Are the Future**\n",
    "While **LSTMs** solve many RNN problems, they still have **sequential dependency** issues. Transformers, using **Self-Attention**, completely **remove recurrence** and allow **parallelization** over long sequences.\n",
    "\n",
    "### **Conclusion**\n",
    "1. **RNNs suffer from exploding/vanishing gradients and long-term dependency issues.**  \n",
    "2. **LSTMs introduce gates to selectively remember or forget information.**  \n",
    "3. **Despite their improvements, LSTMs are still slow and hard to parallelize.**  \n",
    "4. **Transformers overcome LSTM's limitations using Self-Attention.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse Representation** and **Dense Representation** are fundamental concepts in **machine learning**, **deep learning**, and **natural language processing (NLP)**. \n",
    "\n",
    "## **1. Sparse Representation**\n",
    "### **What is it?**\n",
    "- A **sparse representation** stores most values as **zeros**, meaning **only a few elements are nonzero**.\n",
    "- This is useful for data where **most features are irrelevant**, such as **one-hot encoding** or **bag-of-words**.\n",
    "\n",
    "### **Example (One-Hot Encoding)**\n",
    "Imagine you have a vocabulary of **10,000 words**, and you want to represent the word **‚ÄúKrishna‚Äù**. Using **one-hot encoding**, it would look like:\n",
    "\n",
    "$$\n",
    "\\text{Krishna} = [0, 0, 0, 1, 0, ..., 0] \\quad \\text{(10,000-dimensional vector)}\n",
    "$$\n",
    "\n",
    "- **Mostly zeros**, only one **1**.\n",
    "- **Very high-dimensional** but **not memory-efficient**.\n",
    "\n",
    "### **Pros**\n",
    "‚úÖ **Interpretable** (each position has a meaning)  \n",
    "‚úÖ **Preserves uniqueness** (each word has its own slot)  \n",
    "\n",
    "### **Cons**\n",
    "‚ùå **Very high-dimensional** (wastes memory)  \n",
    "‚ùå **Not efficient** for large vocabularies  \n",
    "‚ùå **No relationships** between similar words  \n",
    "\n",
    "\n",
    "## **2. Dense Representation**\n",
    "### **What is it?**\n",
    "- A **dense representation** uses **low-dimensional vectors** where **all values are nonzero**.\n",
    "- This allows for **compact and meaningful representations**, where similar words have **similar vector values**.\n",
    "\n",
    "### **Example (Word Embeddings)**\n",
    "Using **Word2Vec or GloVe**, the word **‚ÄúKrishna‚Äù** might be represented as:\n",
    "\n",
    "$$\n",
    "\\text{Krishna} = [0.32, -0.85, 0.67, ..., 0.12] \\quad \\text{(300-dimensional vector)}\n",
    "$$\n",
    "\n",
    "- **Lower dimensional** (e.g., **300** instead of **10,000**).  \n",
    "- **Contains meaningful patterns** (words with similar meanings have **closer** vectors).  \n",
    "\n",
    "### **Pros**\n",
    "‚úÖ **Efficient** (lower dimensions, better memory usage)  \n",
    "‚úÖ **Encodes relationships** (similar words are closer)  \n",
    "‚úÖ **Works well in deep learning**  \n",
    "\n",
    "### **Cons**\n",
    "‚ùå **Not directly interpretable**  \n",
    "‚ùå **Requires training (e.g., Word2Vec, GloVe, BERT)**  \n",
    "\n",
    "\n",
    "## **3. Key Differences**\n",
    "| Feature | Sparse Representation | Dense Representation |\n",
    "|---------|-----------------|-----------------|\n",
    "| **Dimensions** | Very High | Low |\n",
    "| **Efficiency** | Memory inefficient | Memory efficient |\n",
    "| **Similarity Info** | No relationship | Captures meaning |\n",
    "| **Interpretability** | Easy to interpret | Harder to interpret |\n",
    "| **Example** | One-hot encoding | Word Embeddings |\n",
    "\n",
    "\n",
    "## **4. When to Use What?**\n",
    "üîπ **Use Sparse Representation when:**\n",
    "- Data is naturally **categorical** (e.g., one-hot encoding).\n",
    "- You need **exact representation** (e.g., bag-of-words in text classification).\n",
    "- **Memory is not an issue**.\n",
    "\n",
    "üîπ **Use Dense Representation when:**\n",
    "- You need **semantic meaning** (e.g., NLP embeddings).\n",
    "- Your model must handle **large-scale data** efficiently.\n",
    "- You're working with **deep learning (transformers, LSTMs, CNNs)**.\n",
    "\n",
    "### **Conclusion**\n",
    "- **Sparse representations** are useful when **categorical uniqueness** matters but suffer from **high dimensionality**.  \n",
    "- **Dense representations** provide **efficient, meaningful encodings** and are widely used in **modern AI models** like **transformers** and **deep learning architectures**.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
