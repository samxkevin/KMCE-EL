{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0; Loss:0.6683958172798157\n",
      "Epoch:1; Loss:0.6679306626319885\n",
      "Epoch:2; Loss:0.6674660444259644\n",
      "Epoch:3; Loss:0.6670020222663879\n",
      "Epoch:4; Loss:0.666538655757904\n",
      "Epoch:5; Loss:0.6660757064819336\n",
      "Epoch:6; Loss:0.66561359167099\n",
      "Epoch:7; Loss:0.6651521921157837\n",
      "Epoch:8; Loss:0.6646914482116699\n",
      "Epoch:9; Loss:0.6642311215400696\n",
      "Epoch:10; Loss:0.6637715697288513\n",
      "Epoch:11; Loss:0.6633126139640808\n",
      "Epoch:12; Loss:0.6628545522689819\n",
      "Epoch:13; Loss:0.6623967885971069\n",
      "Epoch:14; Loss:0.6619398593902588\n",
      "Epoch:15; Loss:0.6614835858345032\n",
      "Epoch:16; Loss:0.6610279083251953\n",
      "Epoch:17; Loss:0.66057288646698\n",
      "Epoch:18; Loss:0.6601186394691467\n",
      "Epoch:19; Loss:0.6596648693084717\n",
      "Epoch:20; Loss:0.6592118740081787\n",
      "Epoch:21; Loss:0.6587595343589783\n",
      "Epoch:22; Loss:0.6583078503608704\n",
      "Epoch:23; Loss:0.6578567028045654\n",
      "Epoch:24; Loss:0.6574061512947083\n",
      "Epoch:25; Loss:0.6569562554359436\n",
      "Epoch:26; Loss:0.6565068960189819\n",
      "Epoch:27; Loss:0.6560581922531128\n",
      "Epoch:28; Loss:0.6556100845336914\n",
      "Epoch:29; Loss:0.6551626324653625\n",
      "Epoch:30; Loss:0.6547159552574158\n",
      "Epoch:31; Loss:0.6542698740959167\n",
      "Epoch:32; Loss:0.6538244485855103\n",
      "Epoch:33; Loss:0.6533798575401306\n",
      "Epoch:34; Loss:0.6529359221458435\n",
      "Epoch:35; Loss:0.6524925827980042\n",
      "Epoch:36; Loss:0.6520498991012573\n",
      "Epoch:37; Loss:0.6516077518463135\n",
      "Epoch:38; Loss:0.6511662006378174\n",
      "Epoch:39; Loss:0.6507253050804138\n",
      "Epoch:40; Loss:0.6502850651741028\n",
      "Epoch:41; Loss:0.6498453617095947\n",
      "Epoch:42; Loss:0.6494061946868896\n",
      "Epoch:43; Loss:0.6489677429199219\n",
      "Epoch:44; Loss:0.6485297083854675\n",
      "Epoch:45; Loss:0.6480923891067505\n",
      "Epoch:46; Loss:0.6476556658744812\n",
      "Epoch:47; Loss:0.6472195982933044\n",
      "Epoch:48; Loss:0.6467841267585754\n",
      "Epoch:49; Loss:0.6463490724563599\n",
      "Epoch:50; Loss:0.6459146738052368\n",
      "Epoch:51; Loss:0.6454806923866272\n",
      "Epoch:52; Loss:0.6450474262237549\n",
      "Epoch:53; Loss:0.6446147561073303\n",
      "Epoch:54; Loss:0.6441826820373535\n",
      "Epoch:55; Loss:0.6437512636184692\n",
      "Epoch:56; Loss:0.6433203220367432\n",
      "Epoch:57; Loss:0.6428900361061096\n",
      "Epoch:58; Loss:0.6424602270126343\n",
      "Epoch:59; Loss:0.6420310139656067\n",
      "Epoch:60; Loss:0.6416024565696716\n",
      "Epoch:61; Loss:0.6411744356155396\n",
      "Epoch:62; Loss:0.6407469511032104\n",
      "Epoch:63; Loss:0.6403201222419739\n",
      "Epoch:64; Loss:0.6398937702178955\n",
      "Epoch:65; Loss:0.6394680738449097\n",
      "Epoch:66; Loss:0.6390429139137268\n",
      "Epoch:67; Loss:0.6386182904243469\n",
      "Epoch:68; Loss:0.6381943821907043\n",
      "Epoch:69; Loss:0.6377710103988647\n",
      "Epoch:70; Loss:0.6373484134674072\n",
      "Epoch:71; Loss:0.6369264125823975\n",
      "Epoch:72; Loss:0.6365050673484802\n",
      "Epoch:73; Loss:0.6360841989517212\n",
      "Epoch:74; Loss:0.6356638073921204\n",
      "Epoch:75; Loss:0.6352441906929016\n",
      "Epoch:76; Loss:0.6348249912261963\n",
      "Epoch:77; Loss:0.6344062089920044\n",
      "Epoch:78; Loss:0.633988082408905\n",
      "Epoch:79; Loss:0.6335704326629639\n",
      "Epoch:80; Loss:0.6331532001495361\n",
      "Epoch:81; Loss:0.6327366828918457\n",
      "Epoch:82; Loss:0.6323207020759583\n",
      "Epoch:83; Loss:0.6319053769111633\n",
      "Epoch:84; Loss:0.6314905285835266\n",
      "Epoch:85; Loss:0.6310763955116272\n",
      "Epoch:86; Loss:0.6306628584861755\n",
      "Epoch:87; Loss:0.6302499175071716\n",
      "Epoch:88; Loss:0.6298373341560364\n",
      "Epoch:89; Loss:0.6294252276420593\n",
      "Epoch:90; Loss:0.62901371717453\n",
      "Epoch:91; Loss:0.6286026239395142\n",
      "Epoch:92; Loss:0.6281920075416565\n",
      "Epoch:93; Loss:0.6277819871902466\n",
      "Epoch:94; Loss:0.6273725032806396\n",
      "Epoch:95; Loss:0.6269634962081909\n",
      "Epoch:96; Loss:0.6265549659729004\n",
      "Epoch:97; Loss:0.6261469125747681\n",
      "Epoch:98; Loss:0.6257393956184387\n",
      "Epoch:99; Loss:0.6253324747085571\n",
      "Accuracy:85.09%\n"
     ]
    }
   ],
   "source": [
    "#First: ANN:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data=load_breast_cancer()\n",
    "X=data.data\n",
    "y=data.target\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
    "X_test=torch.tensor(X_test,dtype=torch.float32).to(device)\n",
    "y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
    "y_test=torch.tensor(y_test,dtype=torch.float32).to(device)\n",
    "\n",
    "class Yogeshwar(nn.Module):\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(Yogeshwar,self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,64)\n",
    "        self.fc2=nn.ReLU()\n",
    "        self.fc3=nn.Linear(64,outputs)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        out=self.fc1(x)\n",
    "        out=self.fc2(out)\n",
    "        out=self.fc3(out)\n",
    "        out=self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "inputs=X_train.shape[1]\n",
    "outputs=1\n",
    "model=Yogeshwar(inputs,outputs).to(device)\n",
    "\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs=model(X_train)\n",
    "    loss=criterion(outputs.squeeze(),y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch:{epoch}; Loss:{loss}\")\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs=model(X_test)\n",
    "    predicted=(outputs.squeeze()>0.5).float()\n",
    "    accuracy=accuracy_score(y_test.cpu(),predicted.cpu())\n",
    "    print(f\"Accuracy:{accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epoch 1/100 Loss Accumulated is 0.7018969058990479.\n",
      "For Epoch 2/100 Loss Accumulated is 0.5550190806388855.\n",
      "For Epoch 3/100 Loss Accumulated is 0.4460403025150299.\n",
      "For Epoch 4/100 Loss Accumulated is 0.36130908131599426.\n",
      "For Epoch 5/100 Loss Accumulated is 0.2950375974178314.\n",
      "For Epoch 6/100 Loss Accumulated is 0.24437150359153748.\n",
      "For Epoch 7/100 Loss Accumulated is 0.2063116729259491.\n",
      "For Epoch 8/100 Loss Accumulated is 0.17791946232318878.\n",
      "For Epoch 9/100 Loss Accumulated is 0.15646250545978546.\n",
      "For Epoch 10/100 Loss Accumulated is 0.1398845613002777.\n",
      "For Epoch 11/100 Loss Accumulated is 0.12680982053279877.\n",
      "For Epoch 12/100 Loss Accumulated is 0.11619067937135696.\n",
      "For Epoch 13/100 Loss Accumulated is 0.10735520720481873.\n",
      "For Epoch 14/100 Loss Accumulated is 0.09991349279880524.\n",
      "For Epoch 15/100 Loss Accumulated is 0.09351244568824768.\n",
      "For Epoch 16/100 Loss Accumulated is 0.08796381950378418.\n",
      "For Epoch 17/100 Loss Accumulated is 0.08324123919010162.\n",
      "For Epoch 18/100 Loss Accumulated is 0.07928641140460968.\n",
      "For Epoch 19/100 Loss Accumulated is 0.07604596763849258.\n",
      "For Epoch 20/100 Loss Accumulated is 0.07337488979101181.\n",
      "For Epoch 21/100 Loss Accumulated is 0.0711381584405899.\n",
      "For Epoch 22/100 Loss Accumulated is 0.06924968957901001.\n",
      "For Epoch 23/100 Loss Accumulated is 0.06755933910608292.\n",
      "For Epoch 24/100 Loss Accumulated is 0.06597229838371277.\n",
      "For Epoch 25/100 Loss Accumulated is 0.06441012024879456.\n",
      "For Epoch 26/100 Loss Accumulated is 0.06283929944038391.\n",
      "For Epoch 27/100 Loss Accumulated is 0.061253808438777924.\n",
      "For Epoch 28/100 Loss Accumulated is 0.059643473476171494.\n",
      "For Epoch 29/100 Loss Accumulated is 0.057994384318590164.\n",
      "For Epoch 30/100 Loss Accumulated is 0.056341007351875305.\n",
      "For Epoch 31/100 Loss Accumulated is 0.05477483943104744.\n",
      "For Epoch 32/100 Loss Accumulated is 0.053265273571014404.\n",
      "For Epoch 33/100 Loss Accumulated is 0.051837529987096786.\n",
      "For Epoch 34/100 Loss Accumulated is 0.05048554390668869.\n",
      "For Epoch 35/100 Loss Accumulated is 0.04925621673464775.\n",
      "For Epoch 36/100 Loss Accumulated is 0.04810188710689545.\n",
      "For Epoch 37/100 Loss Accumulated is 0.04702708125114441.\n",
      "For Epoch 38/100 Loss Accumulated is 0.0460364893078804.\n",
      "For Epoch 39/100 Loss Accumulated is 0.04507940262556076.\n",
      "For Epoch 40/100 Loss Accumulated is 0.04416394978761673.\n",
      "For Epoch 41/100 Loss Accumulated is 0.043314170092344284.\n",
      "For Epoch 42/100 Loss Accumulated is 0.04248565062880516.\n",
      "For Epoch 43/100 Loss Accumulated is 0.041661690920591354.\n",
      "For Epoch 44/100 Loss Accumulated is 0.04086851701140404.\n",
      "For Epoch 45/100 Loss Accumulated is 0.040096450597047806.\n",
      "For Epoch 46/100 Loss Accumulated is 0.03933746740221977.\n",
      "For Epoch 47/100 Loss Accumulated is 0.03859180957078934.\n",
      "For Epoch 48/100 Loss Accumulated is 0.037879832088947296.\n",
      "For Epoch 49/100 Loss Accumulated is 0.03717854619026184.\n",
      "For Epoch 50/100 Loss Accumulated is 0.03649624064564705.\n",
      "For Epoch 51/100 Loss Accumulated is 0.035831328481435776.\n",
      "For Epoch 52/100 Loss Accumulated is 0.03518832102417946.\n",
      "For Epoch 53/100 Loss Accumulated is 0.03454923257231712.\n",
      "For Epoch 54/100 Loss Accumulated is 0.03391379117965698.\n",
      "For Epoch 55/100 Loss Accumulated is 0.0332891121506691.\n",
      "For Epoch 56/100 Loss Accumulated is 0.032675985246896744.\n",
      "For Epoch 57/100 Loss Accumulated is 0.032054200768470764.\n",
      "For Epoch 58/100 Loss Accumulated is 0.031437575817108154.\n",
      "For Epoch 59/100 Loss Accumulated is 0.030802646651864052.\n",
      "For Epoch 60/100 Loss Accumulated is 0.0301798265427351.\n",
      "For Epoch 61/100 Loss Accumulated is 0.0295576099306345.\n",
      "For Epoch 62/100 Loss Accumulated is 0.028959142044186592.\n",
      "For Epoch 63/100 Loss Accumulated is 0.028367359191179276.\n",
      "For Epoch 64/100 Loss Accumulated is 0.027775147929787636.\n",
      "For Epoch 65/100 Loss Accumulated is 0.027192382141947746.\n",
      "For Epoch 66/100 Loss Accumulated is 0.02661888487637043.\n",
      "For Epoch 67/100 Loss Accumulated is 0.02605743147432804.\n",
      "For Epoch 68/100 Loss Accumulated is 0.02548111043870449.\n",
      "For Epoch 69/100 Loss Accumulated is 0.024904849007725716.\n",
      "For Epoch 70/100 Loss Accumulated is 0.024331307038664818.\n",
      "For Epoch 71/100 Loss Accumulated is 0.023785622790455818.\n",
      "For Epoch 72/100 Loss Accumulated is 0.023261545225977898.\n",
      "For Epoch 73/100 Loss Accumulated is 0.02273220382630825.\n",
      "For Epoch 74/100 Loss Accumulated is 0.022194668650627136.\n",
      "For Epoch 75/100 Loss Accumulated is 0.021662002429366112.\n",
      "For Epoch 76/100 Loss Accumulated is 0.02118798904120922.\n",
      "For Epoch 77/100 Loss Accumulated is 0.02071179822087288.\n",
      "For Epoch 78/100 Loss Accumulated is 0.020236289128661156.\n",
      "For Epoch 79/100 Loss Accumulated is 0.019758276641368866.\n",
      "For Epoch 80/100 Loss Accumulated is 0.019280249252915382.\n",
      "For Epoch 81/100 Loss Accumulated is 0.018827244639396667.\n",
      "For Epoch 82/100 Loss Accumulated is 0.01835709810256958.\n",
      "For Epoch 83/100 Loss Accumulated is 0.01789700984954834.\n",
      "For Epoch 84/100 Loss Accumulated is 0.017444506287574768.\n",
      "For Epoch 85/100 Loss Accumulated is 0.016986675560474396.\n",
      "For Epoch 86/100 Loss Accumulated is 0.016541562974452972.\n",
      "For Epoch 87/100 Loss Accumulated is 0.016114693135023117.\n",
      "For Epoch 88/100 Loss Accumulated is 0.015669355168938637.\n",
      "For Epoch 89/100 Loss Accumulated is 0.015243819914758205.\n",
      "For Epoch 90/100 Loss Accumulated is 0.014841049909591675.\n",
      "For Epoch 91/100 Loss Accumulated is 0.014442706480622292.\n",
      "For Epoch 92/100 Loss Accumulated is 0.014069942757487297.\n",
      "For Epoch 93/100 Loss Accumulated is 0.013710591942071915.\n",
      "For Epoch 94/100 Loss Accumulated is 0.0133710578083992.\n",
      "For Epoch 95/100 Loss Accumulated is 0.013036512769758701.\n",
      "For Epoch 96/100 Loss Accumulated is 0.012708911672234535.\n",
      "For Epoch 97/100 Loss Accumulated is 0.01239381916821003.\n",
      "For Epoch 98/100 Loss Accumulated is 0.012080471962690353.\n",
      "For Epoch 99/100 Loss Accumulated is 0.011777660809457302.\n",
      "For Epoch 100/100 Loss Accumulated is 0.011483747512102127.\n",
      "Accuracy: 98.25\n"
     ]
    }
   ],
   "source": [
    "#Practicing ANN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from  sklearn.metrics import accuracy_score\n",
    "\n",
    "data=load_breast_cancer()\n",
    "X=data.data\n",
    "y=data.target\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n",
    "\n",
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
    "X_test=torch.tensor(X_test,dtype=torch.float32).to(device)\n",
    "y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
    "y_test=torch.tensor(y_test,dtype=torch.float32).to(device)\n",
    "\n",
    "class Yogeshwar(nn.Module):\n",
    "    def __init__(self,inputs,hidden,outputs):\n",
    "        super(Yogeshwar,self).__init__()\n",
    "        self.fc1=nn.Linear(inputs,hidden)\n",
    "        self.fc2=nn.ReLU()\n",
    "        self.fc3=nn.Linear(hidden,outputs)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        tat=self.fc1(x)\n",
    "        tat=self.fc2(tat)\n",
    "        tat=self.fc3(tat)\n",
    "        tat=self.sigmoid(tat)\n",
    "        return tat\n",
    "    \n",
    "inputs=X_train.shape[1]\n",
    "hidden=64\n",
    "outputs=1\n",
    "\n",
    "model=Yogeshwar(inputs,hidden,outputs).to(device)\n",
    "\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs=model(X_train)\n",
    "    loss=criterion(outputs.squeeze(),y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"For Epoch {epoch+1}/{epochs} Loss Accumulated is {loss}.\")\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs=model(X_test)\n",
    "    predicted=(outputs.squeeze()>0.5).float()\n",
    "    accuracy=accuracy_score(y_test.cpu(),predicted.cpu())\n",
    "    print(f\"Accuracy: {accuracy*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.0777\n",
      "Epoch 11/100, Loss: 0.0192\n",
      "Epoch 21/100, Loss: 0.0059\n",
      "Epoch 31/100, Loss: 0.0030\n",
      "Epoch 41/100, Loss: 0.0028\n",
      "Epoch 51/100, Loss: 0.0018\n",
      "Epoch 61/100, Loss: 0.0013\n",
      "Epoch 71/100, Loss: 0.0012\n",
      "Epoch 81/100, Loss: 0.0023\n",
      "Epoch 91/100, Loss: 0.0020\n",
      "\n",
      " ========EVALUATION======== \n",
      "\n",
      "Predicted:402.94,Actual:359.00\n",
      "Predicted:361.60,Actual:310.00\n",
      "Predicted:338.92,Actual:337.00\n",
      "Predicted:380.33,Actual:360.00\n",
      "Predicted:395.07,Actual:342.00\n",
      "Predicted:375.90,Actual:406.00\n",
      "Predicted:386.53,Actual:396.00\n",
      "Predicted:381.16,Actual:420.00\n",
      "Predicted:468.43,Actual:472.00\n",
      "Predicted:508.76,Actual:548.00\n",
      "Predicted:542.94,Actual:559.00\n",
      "Predicted:503.10,Actual:463.00\n",
      "Predicted:427.74,Actual:407.00\n",
      "Predicted:391.43,Actual:362.00\n",
      "Predicted:350.37,Actual:405.00\n",
      "Predicted:419.30,Actual:417.00\n",
      "Predicted:425.17,Actual:391.00\n",
      "Predicted:428.38,Actual:419.00\n",
      "Predicted:410.15,Actual:461.00\n",
      "Predicted:435.94,Actual:472.00\n",
      "Predicted:504.24,Actual:535.00\n",
      "Predicted:553.55,Actual:622.00\n",
      "Predicted:587.13,Actual:606.00\n",
      "Predicted:532.78,Actual:508.00\n",
      "Predicted:471.83,Actual:461.00\n",
      "Predicted:436.42,Actual:390.00\n",
      "Predicted:420.00,Actual:432.00\n",
      "\n",
      "======Model Accuracy=========\n",
      "\n",
      "Model Accuracy: 93.04%\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=sns.load_dataset(\"flights\")\n",
    "\n",
    "df['date']=pd.to_datetime(df['year'].astype(str)+'-'+df['month'].astype(str),format=\"%Y-%b\")\n",
    "\n",
    "data=df[['date','passengers']]\n",
    "data.set_index('date',inplace=True)\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "data=data.copy()\n",
    "data['passengers']=scaler.fit_transform(data[['passengers']]).astype(float)\n",
    "\n",
    "SequenceLength=12\n",
    "\n",
    "def SequenceCreation(data,seq_length):\n",
    "    X,y=[],[]\n",
    "    for i in range(len(data)-seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X),np.array(y)\n",
    "\n",
    "X,y=SequenceCreation(data['passengers'].values,SequenceLength)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
    "y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
    "X_test=torch.tensor(X_test,dtype=torch.float32).to(device)\n",
    "y_test=torch.tensor(y_test,dtype=torch.float32).to(device)\n",
    "\n",
    "class YogeshwaraRNN(nn.Module):\n",
    "    def __init__(self,inp,sec,out):\n",
    "        super(YogeshwaraRNN,self).__init__()\n",
    "        self.RNN=nn.RNN(inp,sec,batch_first=True)\n",
    "        self.fc=nn.Linear(sec,out)   \n",
    "    def forward(self,x):\n",
    "        out,_=self.RNN(x)\n",
    "        out=self.fc(out[:,-1,:])\n",
    "        return out\n",
    "\n",
    "inp=1  \n",
    "sec=64  \n",
    "out=1  \n",
    "\n",
    "model=YogeshwaraRNN(inp,sec,out).to(device)\n",
    "\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.01)\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs=model(X_train.unsqueeze(-1))\n",
    "    loss=criterion(outputs.squeeze(),y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\n ========EVALUATION======== \\n\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions=model(X_test.unsqueeze(-1)).squeeze()\n",
    "predictions=scaler.inverse_transform(predictions.cpu().numpy().reshape(-1,1))\n",
    "y_test_original=scaler.inverse_transform(y_test.cpu().numpy().reshape(-1,1))\n",
    "for pred,actual in zip(predictions.ravel(),y_test_original.ravel()):\n",
    "    print(f\"Predicted:{pred:.2f},Actual:{actual:.2f}\")\n",
    "print(\"\\n======Model Accuracy=========\\n\")\n",
    "mape=np.mean(np.abs((y_test_original-predictions)/y_test_original))*100\n",
    "print(f\"Model Accuracy: {100 - mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.0588\n",
      "Epoch 11/100, Loss: 0.0185\n",
      "Epoch 21/100, Loss: 0.0142\n",
      "Epoch 31/100, Loss: 0.0083\n",
      "Epoch 41/100, Loss: 0.0030\n",
      "Epoch 51/100, Loss: 0.0026\n",
      "Epoch 61/100, Loss: 0.0022\n",
      "Epoch 71/100, Loss: 0.0019\n",
      "Epoch 81/100, Loss: 0.0016\n",
      "Epoch 91/100, Loss: 0.0014\n",
      "\n",
      " ========EVALUATION======== \n",
      "\n",
      "Predicted:394.40,Actual:359.00\n",
      "Predicted:333.56,Actual:310.00\n",
      "Predicted:327.47,Actual:337.00\n",
      "Predicted:379.35,Actual:360.00\n",
      "Predicted:377.34,Actual:342.00\n",
      "Predicted:350.63,Actual:406.00\n",
      "Predicted:358.14,Actual:396.00\n",
      "Predicted:377.02,Actual:420.00\n",
      "Predicted:464.83,Actual:472.00\n",
      "Predicted:491.27,Actual:548.00\n",
      "Predicted:524.18,Actual:559.00\n",
      "Predicted:486.74,Actual:463.00\n",
      "Predicted:417.82,Actual:407.00\n",
      "Predicted:361.81,Actual:362.00\n",
      "Predicted:332.64,Actual:405.00\n",
      "Predicted:416.18,Actual:417.00\n",
      "Predicted:409.05,Actual:391.00\n",
      "Predicted:400.65,Actual:419.00\n",
      "Predicted:377.03,Actual:461.00\n",
      "Predicted:424.29,Actual:472.00\n",
      "Predicted:505.34,Actual:535.00\n",
      "Predicted:534.73,Actual:622.00\n",
      "Predicted:561.89,Actual:606.00\n",
      "Predicted:514.97,Actual:508.00\n",
      "Predicted:462.80,Actual:461.00\n",
      "Predicted:405.43,Actual:390.00\n",
      "Predicted:399.12,Actual:432.00\n",
      "\n",
      "======Model Accuracy=========\n",
      "\n",
      "Model Accuracy: 92.93%\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=sns.load_dataset(\"flights\")\n",
    "\n",
    "df['date']=pd.to_datetime(df['year'].astype(str)+'-'+df['month'].astype(str),format=\"%Y-%b\")\n",
    "\n",
    "data=df[['date','passengers']]\n",
    "data.set_index('date',inplace=True)\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "data=data.copy()\n",
    "data['passengers']=scaler.fit_transform(data[['passengers']]).astype(float)\n",
    "\n",
    "SequenceLength=12\n",
    "\n",
    "def SequenceCreation(data,seq_length):\n",
    "    X,y=[],[]\n",
    "    for i in range(len(data)-seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X),np.array(y)\n",
    "\n",
    "X,y=SequenceCreation(data['passengers'].values,SequenceLength)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train=torch.tensor(X_train,dtype=torch.float32).to(device)\n",
    "y_train=torch.tensor(y_train,dtype=torch.float32).to(device)\n",
    "X_test=torch.tensor(X_test,dtype=torch.float32).to(device)\n",
    "y_test=torch.tensor(y_test,dtype=torch.float32).to(device)\n",
    "\n",
    "class YogeshwaraRNN(nn.Module):\n",
    "    def __init__(self,inp,sec,out):\n",
    "        super(YogeshwaraRNN,self).__init__()\n",
    "        self.RNN=nn.RNN(inp,sec,batch_first=True)\n",
    "        self.fc=nn.Linear(sec,out)   \n",
    "    def forward(self,x):\n",
    "        tat,_=self.RNN(x)\n",
    "        tat=self.fc(tat[:,-1,:])\n",
    "        return tat\n",
    "\n",
    "inp=1  \n",
    "sec=64  \n",
    "out=1  \n",
    "\n",
    "model=YogeshwaraRNN(inp,sec,out).to(device)\n",
    "\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=0.01)\n",
    "\n",
    "epochs=100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs=model(X_train.unsqueeze(-1))\n",
    "    loss=criterion(outputs.squeeze(),y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "print(\"\\n ========EVALUATION======== \\n\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions=model(X_test.unsqueeze(-1)).squeeze()\n",
    "predictions=scaler.inverse_transform(predictions.cpu().numpy().reshape(-1,1))\n",
    "y_test_original=scaler.inverse_transform(y_test.cpu().numpy().reshape(-1,1))\n",
    "for pred,actual in zip(predictions.ravel(),y_test_original.ravel()):\n",
    "    print(f\"Predicted:{pred:.2f},Actual:{actual:.2f}\")\n",
    "print(\"\\n======Model Accuracy=========\\n\")\n",
    "mape=np.mean(np.abs((y_test_original-predictions)/y_test_original))*100\n",
    "print(f\"Model Accuracy: {100 - mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
