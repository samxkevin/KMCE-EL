{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning can be callled as Function Approximation. Single perceptron can only handle linear problems so it is not feasible for Real World Problem as there is noise when visualizing it.\n",
    "\n",
    "Deep Neural Network when we have many hidden layers. If Every input is connected to every neuron in a neural network then it is called as a Deep neural network.\n",
    "\n",
    "Computation in MLP: Check\n",
    "\n",
    "No. of weights is equalt to the number of neurons multiplied by number of inputs.\n",
    "\n",
    "In some cases we transpose weights before itself. We use activation function to capture multiple linear functions in our data. Activation function which are non linear are necessary in this case. We are trying to learn the relationship between input and output in the Data.\n",
    "\n",
    "It is always better to compute object oriented because it is always extensible and does not require changing the functional code. Linear problems are given to the Activation Function.Which are dense in nature. We put the common functionality to Base class which shall be implemented for all the core. \n",
    "\n",
    "We do backpropagation for finding the loss at node at last. Error is propagated from the back side. Error is calculated at the higher and is propagated back.\n",
    "\n",
    "Non- Linear Activation functions- check...\n",
    "\n",
    "Tanh is zero centric but sigmoid what is the drawback explain?\n",
    "\n",
    "We use Softmax activation function for Multi class classification. We have momentum to escape from saddle points. When one of the feature is sparse then we use adagrad. How does momentum gradient escape saddle point. Why should we have different weights with different gradients in multi layer perceptron. rmsprop?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
