{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Continution of Yesterdays's Class by Haribabu Sir__:\n",
    "\n",
    "# __Comparison of the Four Gradient Descent Methods:__  \n",
    "\n",
    "### 1. **Standard (Batch) Gradient Descent**  \n",
    "- **Definition**: Computes the gradient using the entire dataset.  \n",
    "- **Update Rule**:  \n",
    "Formula:  \n",
    "$$ w := w - \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)}) \\cdot x^{(i)} $$  \n",
    "\n",
    "#### Terms:\n",
    "1. **$w$**: Weight (parameter) being optimized.\n",
    "2. **$\\eta$ (Learning Rate)**: Controls the size of the step in the direction of the negative gradient.\n",
    "3. **$n$**: Total number of training samples in the dataset.\n",
    "4. **$(y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)})$**: The difference between the predicted and true value for sample $i$ (error term).\n",
    "5. **$x^{(i)}$**: Input feature corresponding to sample $i$.\n",
    "6. **$\\sum_{i=1}^{n}$**: Summation over all $n$ samples in the dataset.\n",
    "\n",
    "**Key Property:** Uses the entire dataset for each update, ensuring accurate but slow updates.\n",
    "\n",
    "- **Pros**: Accurate updates.  \n",
    "- **Cons**: Slow for large datasets.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Stochastic Gradient Descent (SGD)**  \n",
    "- **Definition**: Uses a single random data point for each update.  \n",
    "- **Update Rule**:  \n",
    "Formula:  \n",
    "$$ w := w - \\eta \\cdot (y_{\\text{pred}} - y_{\\text{true}}) \\cdot x $$  \n",
    "\n",
    "#### Terms:\n",
    "1. **$w$**: Weight (parameter) being updated.\n",
    "2. **$\\eta$**: Learning rate.\n",
    "3. **$(y_{\\text{pred}} - y_{\\text{true}})$**: Error term for a single sample.\n",
    "4. **$x$**: Input feature for the single sample being used for the update.\n",
    "\n",
    "**Key Property:** Updates weights for each individual sample, making it faster but noisier.\n",
    "\n",
    "- **Pros**: Fast updates, good for large datasets.  \n",
    "- **Cons**: Noisy convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Mini-Batch Gradient Descent**  \n",
    "- **Definition**: Uses small batches of data (size $n$) for each update.  \n",
    "- **Update Rule**:  \n",
    "Formula:  \n",
    "$$ w := w - \\eta \\cdot \\frac{1}{b} \\sum_{j=1}^{b} (y_{\\text{pred}}^{(j)} - y_{\\text{true}}^{(j)}) \\cdot x^{(j)} $$  \n",
    "\n",
    "#### Terms:\n",
    "1. **$b$**: Batch size (number of samples in a mini-batch).  \n",
    "2. **$\\sum_{j=1}^{b}$**: Summation over all $b$ samples in the mini-batch.  \n",
    "3. Other terms ($w$, $\\eta$, $x$, $y_{\\text{pred}}$, $y_{\\text{true}}$) are as defined above.\n",
    "\n",
    "**Key Property:** Uses a subset of data (mini-batch) for each update, balancing stability and efficiency.\n",
    "\n",
    "- **Pros**: Balances stability and speed, hardware-efficient.  \n",
    "- **Cons**: Requires choosing a proper batch size.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Momentum Gradient Descent**  \n",
    "- **Definition**: Incorporates the past update direction for smoother and faster convergence.  \n",
    "- **Update Rule**:  \n",
    "Formulas:  \n",
    "1. Velocity update:  \n",
    "   $$ v_t := \\gamma \\cdot v_{t-1} + \\eta \\cdot \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)}) \\cdot x^{(i)} $$  \n",
    "2. Weight update:  \n",
    "   $$ w := w - v_t $$  \n",
    "\n",
    "#### Terms:\n",
    "1. **$v_t$**: Velocity at time step $t$, which incorporates past gradients.\n",
    "2. **$\\gamma$ (\n",
    "   Momentum Coefficient): Determines the contribution of past gradients (e.g., $\\gamma = 0.9$).\n",
    "3. **$v_{t-1}$**: Velocity from the previous update step.\n",
    "4. Other terms ($w$, $\\eta$, $x$, $y_{\\text{pred}}$, $y_{\\text{true}}$) are as defined above.\n",
    "\n",
    "**Key Property:** Introduces a momentum term to accelerate convergence, particularly in regions with oscillations.\n",
    "\n",
    "- **Pros**: Reduces oscillations, accelerates convergence.  \n",
    "- **Cons**: Requires tuning $\\gamma$.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences:  \n",
    "- **Batch**: Stable but slow.  \n",
    "- **SGD**: Fast but noisy.  \n",
    "- **Mini-Batch**: Balanced approach.  \n",
    "- **Momentum**: Faster convergence by leveraging direction history.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # __Differences Between _Loss Function_ and _Gradient Descent_ :__  \n",
    "\n",
    "### **Loss Function**\n",
    "- **Definition**: A mathematical function that measures the error between predicted values and actual values.  \n",
    "- **Purpose**: Quantifies how well the model is performing.  \n",
    "- **Examples**:  \n",
    "  - Mean Squared Error (MSE): $$L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$  \n",
    "  - Cross-Entropy Loss: $$L = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right)$$  \n",
    "- **Role**: Determines *what* to minimize.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient Descent**\n",
    "- **Definition**: An optimization algorithm used to minimize the loss function by updating model parameters.  \n",
    "- **Purpose**: Finds the values of model parameters ($\\theta$) that minimize the loss.  \n",
    "- **Update Rule**:  \n",
    "  $$\\theta = \\theta - \\alpha \\nabla J(\\theta)$$  \n",
    "  where $\\alpha$ is the learning rate.  \n",
    "- **Role**: Provides the *how* to minimize the loss.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Difference:  \n",
    "- **Loss Function**: Defines the objective to minimize (error).  \n",
    "- **Gradient Descent**: Provides the mechanism to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimization:__\n",
    "\n",
    "__Step - 1__: _Gradient Calcution: slope with direction._\n",
    "\n",
    "__Step - 2__: _Updating the parameters._\n",
    "\n",
    "__Step - 3__: _Repeat the Process from Step 3 in Previous Lecture Until Defined Epochs or decreased Loss._\n",
    "\n",
    "_Note: Remember this is Single Perceptron._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in Classification Problems we use Activation functions such as Sigmoid, which takes the values tending towrds +ve infinity it considers them as One and the Values tending towards -ve infinity as Zero.\n",
    "\n",
    "# __Why not MSE in Classification Problems:__\n",
    "We don't typically use **Mean Squared Error (MSE)** for classification tasks because:  \n",
    "\n",
    "### 1. **Inefficiency in Capturing Probabilities**  \n",
    "- MSE calculates the squared difference between predicted and true labels.  \n",
    "- Classification often involves probabilities (e.g., from softmax or sigmoid), and MSE is not ideal for measuring the divergence between distributions.\n",
    "\n",
    "### 2. **Gradient Saturation**  \n",
    "- For models like logistic regression or neural networks, MSE gradients tend to saturate (become very small) as predictions approach the true label, slowing down learning.\n",
    "\n",
    "### 3. **Better Alternatives Exist**  \n",
    "- **Cross-Entropy Loss** aligns better with probabilistic models, directly penalizing incorrect predictions by comparing predicted probabilities to true labels:\n",
    "  $$L = -\\frac{1}{n} \\sum_{i=1}^n \\left( y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i) \\right)$$  \n",
    "\n",
    "__Additionals__: Saddle points (convex), potential of impact is minimized since in MSE the values recorded are between zero and one.\n",
    "\n",
    "### Summary:  \n",
    "MSE is less effective for classification because it doesn't handle probabilities well, leads to slow convergence, and lacks the efficiency of cross-entropy for penalizing wrong predictions.\n",
    "\n",
    "\n",
    "In classification tasks, predictions typically involve probabilities—values between 0 and 1—that represent the likelihood of a data point belonging to a specific class. For example, in binary classification, a model might output $0.9$ for \"class 1\" and $0.1$ for \"class 0.\"\n",
    "\n",
    "Now, let's understand **why MSE is not ideal** for this setup:  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Probabilities Are Not Linear**  \n",
    "- MSE treats the difference between predicted probabilities and actual labels linearly. For instance, if the true label is 1, MSE penalizes a prediction of $0.5$ the same way as it would penalize $0.9$ (distance squared).  \n",
    "- However, in classification, probabilities near $1$ or $0$ are much more meaningful than intermediate values, so this linear treatment is not suitable.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "__The most suitable quantity in this case is _Entropy_. Entropy is the degree of suprise. Less entropy the better. we use Cross Entropy because there are multiple values.__\n",
    "\n",
    "### 2. **Cross-Entropy Fits Better for Probabilities**  \n",
    "- **Cross-Entropy Loss** directly compares the predicted probability distribution to the true label's one-hot encoded distribution.  \n",
    "- It penalizes incorrect confident predictions more harshly and rewards correct confident predictions effectively.  \n",
    "\n",
    "For example:  \n",
    "- True label: $y = 1$  \n",
    "- Prediction: $\\hat{y} = 0.9$ (confidence is high and correct).  \n",
    "    - Cross-Entropy Loss: Very small penalty.  \n",
    "    - MSE: Still penalizes moderately because $(1 - 0.9)^2 = 0.01$.  \n",
    "- Prediction: $\\hat{y} = 0.5$ (low confidence).  \n",
    "    - Cross-Entropy Loss: Harsh penalty.  \n",
    "    - MSE: Moderate penalty, which doesn't push the model as strongly to improve.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Divergence Between Distributions**  \n",
    "- Classification is about matching probability distributions (true labels vs. predicted probabilities).  \n",
    "- Cross-Entropy Loss measures the divergence between these distributions (how far apart they are). MSE fails to do this effectively because it isn't designed for probabilistic outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Comparison (Binary Classification)  \n",
    "True label: $y = 1$  \n",
    "Predicted probabilities: $\\hat{y} = 0.9$ vs. $\\hat{y} = 0.5$  \n",
    "\n",
    "| Prediction | Cross-Entropy Loss | MSE       |  \n",
    "|------------|---------------------|-----------|  \n",
    "| $\\hat{y} = 0.9$ | $-1 \\cdot \\log(0.9) = 0.105$ | $(1 - 0.9)^2 = 0.01$ |  \n",
    "| $\\hat{y} = 0.5$ | $-1 \\cdot \\log(0.5) = 0.693$ | $(1 - 0.5)^2 = 0.25$ |  \n",
    "\n",
    "- Cross-Entropy Loss sharply distinguishes the predictions, pushing the model to get closer to confident probabilities.  \n",
    "- MSE provides less sharp gradients, making it harder for the model to improve efficiently.  \n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion  \n",
    "MSE is not ideal for classification because it treats probabilities linearly, doesn't effectively penalize confident wrong predictions, and fails to measure the divergence between probability distributions. Cross-Entropy Loss, on the other hand, is better suited for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "𝐇𝐀𝐑𝐄 𝐊𝐑𝐈𝐒𝐇𝐍𝐀  \n",
    "\n",
    "In the context of classification tasks, **divergence between probability distributions** refers to how different the predicted probabilities of a model are from the true probabilities (or true labels in one-hot encoding). Let's break this down:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Probability Distributions in Classification**\n",
    "- In classification, the model predicts a probability distribution over possible classes.  \n",
    "  For example, in binary classification:  \n",
    "  - True label: $y = [1, 0]$ (class 1 is true).  \n",
    "  - Model prediction: $\\hat{y} = [0.9, 0.1]$ (90% confidence for class 1, 10% for class 2).  \n",
    "\n",
    "- The goal is to make the predicted distribution ($\\hat{y}$) as close as possible to the true distribution ($y$).  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Why MSE Fails Here**\n",
    "- **MSE** computes the squared difference between each element of the two distributions:  \n",
    "  $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$  \n",
    "\n",
    "- This approach doesn't effectively capture the relationship between probabilities because it treats all differences linearly and focuses on minimizing squared errors rather than the \"distance\" between the distributions.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **How Cross-Entropy Works**\n",
    "- **Cross-Entropy Loss** is based on a concept from information theory and measures the \"distance\" between two probability distributions. It computes how much information (in bits) is needed to describe the true distribution using the predicted distribution.  \n",
    "  $$L = -\\sum_{i=1}^{n} y_i \\cdot \\log(\\hat{y}_i)$$  \n",
    "\n",
    "- In classification:  \n",
    "  - If $\\hat{y}_i$ (predicted probability for the true class) is close to $1$, the loss is small.  \n",
    "  - If $\\hat{y}_i$ is far from $1$, the loss increases rapidly, penalizing confident but wrong predictions.  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Example**\n",
    "Let’s take two cases for a binary classification:  \n",
    "- True label: $y = [1, 0]$  \n",
    "- Predictions:  \n",
    "  1. $\\hat{y} = [0.9, 0.1]$ (close to the true label).  \n",
    "  2. $\\hat{y} = [0.5, 0.5]$ (less confident, wrong direction).  \n",
    "\n",
    "#### Loss Calculation:  \n",
    "1. **MSE**:  \n",
    "   $$\\text{MSE} = \\frac{1}{2} \\left[(1 - 0.9)^2 + (0 - 0.1)^2\\right] = 0.01 + 0.01 = 0.02$$  \n",
    "   $$\\text{MSE} = \\frac{1}{2} \\left[(1 - 0.5)^2 + (0 - 0.5)^2\\right] = 0.25 + 0.25 = 0.5$$  \n",
    "\n",
    "2. **Cross-Entropy Loss**:  \n",
    "   $$L = -\\left[1 \\cdot \\log(0.9) + 0 \\cdot \\log(0.1)\\right] = -\\log(0.9) = 0.105$$  \n",
    "   $$L = -\\left[1 \\cdot \\log(0.5) + 0 \\cdot \\log(0.5)\\right] = -\\log(0.5) = 0.693$$  \n",
    "\n",
    "#### Observations:  \n",
    "- **MSE** penalizes both predictions but doesn't emphasize the confident wrong predictions strongly enough.  \n",
    "- **Cross-Entropy Loss** penalizes the less confident prediction ($\\hat{y} = 0.5$) much more heavily, encouraging the model to improve probabilistic accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Summary of Divergence**\n",
    "- MSE minimizes the squared differences, which doesn't directly address how \"far apart\" two distributions are.  \n",
    "- Cross-Entropy Loss measures the divergence, prioritizing confident, correct predictions and penalizing confident wrong predictions harshly.  \n",
    "- This makes **Cross-Entropy Loss** the preferred choice for classification tasks involving probabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
