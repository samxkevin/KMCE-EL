{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __TEXT ANALYSIS__\n",
    "## PRE-PROCESSING:\n",
    "In **Deep Learning for Text Analysis**, preprocessing is a crucial step that ensures raw text data is transformed into a format that models can understand. Preprocessing cleans, normalizes, and represents the text while preserving the context and meaning.\n",
    "\n",
    "### **1. Why Preprocessing is Necessary**\n",
    "Text data is inherently unstructured. Preprocessing:\n",
    "- Removes noise (e.g., special characters, stopwords).\n",
    "- Normalizes data for consistency.\n",
    "- Converts text into numerical form (since models work with numbers).\n",
    "\n",
    "\n",
    "\n",
    "### **2. Steps in Text Preprocessing**\n",
    "\n",
    "#### **2.1. Text Cleaning**\n",
    "- **Objective:** Remove irrelevant or noisy parts of text to focus on meaningful content.\n",
    "- **Steps:**\n",
    "  1. **Lowercasing:** Normalize text by converting all characters to lowercase.\n",
    "     - Example: \"Deep Learning is FUN\" → \"deep learning is fun\".\n",
    "  2. **Remove Punctuation & Special Characters:** Remove symbols that don’t add meaning.\n",
    "     - Example: \"Hello! How are you?\" → \"hello how are you\".\n",
    "  3. **Remove Numbers (Optional):** If numbers don’t contribute to the task.\n",
    "  4. **Remove URLs/HTML Tags:** For web text.\n",
    "\n",
    "#### **2.2. Tokenization**\n",
    "- **Objective:** Split text into smaller units like words or sentences.\n",
    "- **Methods:**\n",
    "  - **Word Tokenization:** Breaks text into individual words.\n",
    "    - Example: \"Deep learning is amazing\" → [\"deep\", \"learning\", \"is\", \"amazing\"]\n",
    "  - **Sentence Tokenization:** Breaks text into sentences.\n",
    "    - Example: \"Deep learning is amazing. It is powerful.\" → [\"Deep learning is amazing.\", \"It is powerful.\"]\n",
    "\n",
    "#### **2.3. Stopword Removal**\n",
    "- **Objective:** Remove common words (e.g., \"is\", \"the\", \"a\") that don’t carry significant meaning.\n",
    "- **Example:**\n",
    "  - Input: \"This is an example of text preprocessing.\"\n",
    "  - Output: [\"example\", \"text\", \"preprocessing\"]\n",
    "\n",
    "#### **2.4. Lemmatization and Stemming**\n",
    "- **Objective:** Reduce words to their base or root form.\n",
    "  1. **Stemming:** Strips suffixes.\n",
    "     - Example: \"running\", \"runs\" → \"run\".\n",
    "  2. **Lemmatization:** Maps words to their dictionary form.\n",
    "     - Example: \"better\" → \"good\".\n",
    "\n",
    "#### **2.5. Handling Out-of-Vocabulary (OOV) Words**\n",
    "- Replace rare or unknown words with a placeholder like `<UNK>`.\n",
    "\n",
    "#### **2.6. Handling Misspellings**\n",
    "- Use spell-correction libraries like **SymSpell** or **TextBlob** to correct typos.\n",
    "\n",
    "#### **2.7. Padding**\n",
    "- Deep learning models often require inputs of the same length. Shorter sequences are padded with zeros.\n",
    "  - Example: Input: [\"deep\", \"learning\"], Target Length: 5 → [\"deep\", \"learning\", 0, 0, 0].\n",
    "\n",
    "#### **2.8. Encoding**\n",
    "Once text is preprocessed, it needs to be converted into numerical form:\n",
    "- **Bag of Words (BoW):**\n",
    "  - Creates a vocabulary of unique words, and represents sentences as binary vectors or word counts.\n",
    "  - Example: \"I love NLP\" → [1, 1, 1, 0] (if vocabulary = [\"I\", \"love\", \"NLP\", \"deep\"])\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "  - Weighs word importance based on its frequency across documents.\n",
    "- **Word Embeddings:** Dense, fixed-size vector representations like **Word2Vec**, **GloVe**, or embeddings learned through **transformers** like **BERT**.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Preprocessing Libraries**\n",
    "Here are common libraries used for text preprocessing:\n",
    "1. **NLTK (Natural Language Toolkit):** For tokenization, stopword removal, stemming, and lemmatization.\n",
    "2. **spaCy:** Faster and supports tokenization, lemmatization, and named entity recognition.\n",
    "3. **TextBlob:** Simplifies basic NLP tasks like sentiment analysis and spell-checking.\n",
    "4. **Hugging Face Transformers:** For advanced preprocessing using pretrained transformer models.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Python Example**\n",
    "\n",
    "#### Import Libraries:\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "```\n",
    "\n",
    "#### Preprocessing Steps:\n",
    "```python\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"Deep learning is amazing! Learn more at https://example.com.\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(\"Processed Text:\", processed_text)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Processed Text: ['deep', 'learning', 'amazing', 'learn']\n",
    "```\n",
    "### **5. Challenges in Preprocessing**\n",
    "1. **Context Preservation:** Removing stopwords might lose context (e.g., \"not good\" → [\"good\"] loses the negation).\n",
    "2. **Language-Specific Rules:** Preprocessing pipelines need adaptation for non-English text.\n",
    "3. **Advanced Contexts:** Techniques like stemming/lemmatization might not fully preserve meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Structured Data**\n",
    "#### **Definition:**\n",
    "Structured data refers to data that is organized into predefined formats like rows and columns, making it easily searchable and processable.\n",
    "\n",
    "#### **Characteristics:**\n",
    "- **Format:** Tabular, with clearly defined fields (e.g., spreadsheets, relational databases).\n",
    "- **Schema:** Follows a strict schema, where data types (e.g., integer, float, string) are fixed.\n",
    "- **Storage:** Easily stored in databases (SQL, Excel, etc.).\n",
    "- **Examples:**\n",
    "  - Customer data: Names, ages, addresses, and phone numbers.\n",
    "  - Sensor readings: Time, temperature, and pressure values.\n",
    "  - Transactions: Date, amount, and location.\n",
    "\n",
    "#### **Use Case in AI:**\n",
    "- Used in machine learning models where features and labels are clearly defined.\n",
    "- **Example:** Predicting house prices using features like square footage, number of bedrooms, etc.\n",
    "\n",
    "### **2. Unstructured Data**\n",
    "#### **Definition:**\n",
    "Unstructured data refers to data that doesn’t have a predefined format or schema, making it more difficult to store, process, and analyze.\n",
    "\n",
    "#### **Characteristics:**\n",
    "- **Format:** Free-form; lacks a fixed structure.\n",
    "- **Complexity:** Requires preprocessing to extract meaningful insights.\n",
    "- **Storage:** Stored as files or objects (e.g., in NoSQL databases, Hadoop, cloud storage).\n",
    "- **Examples:**\n",
    "  - **Text data:** Emails, articles, social media posts, chat logs.\n",
    "  - **Multimedia data:** Images, videos, audio recordings.\n",
    "  - **Logs:** Server logs, web activity logs.\n",
    "\n",
    "#### **Unstructured Text Data in NLP:**\n",
    "- Text data is inherently unstructured because:\n",
    "  - It contains raw text with no predefined delimiters.\n",
    "  - Words, sentences, or paragraphs have no inherent numeric representation.\n",
    "\n",
    "### **3. Key Differences Between Structured and Unstructured Data**\n",
    "\n",
    "| **Aspect**        | **Structured Data**                          | **Unstructured Data**                         |\n",
    "|--||-|\n",
    "| **Format**         | Organized into rows and columns             | Free-form, with no predefined schema         |\n",
    "| **Examples**       | Tables, relational databases                | Emails, social media posts, images           |\n",
    "| **Processing**     | Easy to process and analyze                 | Requires preprocessing and feature extraction|\n",
    "| **Tools**          | SQL, Pandas, Excel                          | NLP libraries (spaCy, NLTK), cloud storage   |\n",
    "\n",
    "### **4. What is Natural Language Processing (NLP)?**\n",
    "\n",
    "#### **Definition:**\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that enables computers to understand, interpret, and generate human language.\n",
    "\n",
    "#### **4.1. Components of NLP**\n",
    "\n",
    "1. **Text Preprocessing:**\n",
    "   - Converts raw text into a usable format.\n",
    "   - Includes tokenization, stopword removal, stemming/lemmatization, etc.\n",
    "\n",
    "2. **Syntax Analysis (Parsing):**\n",
    "   - Analyzes sentence structure (e.g., part-of-speech tagging, dependency parsing).\n",
    "   - **Example:** Identify nouns, verbs, and adjectives in a sentence.\n",
    "\n",
    "3. **Semantics (Meaning Extraction):**\n",
    "   - Analyzes the meaning of text.\n",
    "   - Techniques like word embeddings (Word2Vec, GloVe) or transformer-based models (BERT, GPT).\n",
    "\n",
    "4. **Text Representation:**\n",
    "   - Converts text into numeric forms for ML/DL models.\n",
    "   - Techniques include Bag of Words, TF-IDF, Word2Vec, and Sentence Transformers.\n",
    "\n",
    "5. **Applications in ML/DL:**\n",
    "   - Sentiment analysis, topic modeling, language generation.\n",
    "\n",
    "#### **4.2. Applications of NLP**\n",
    "\n",
    "1. **Text Classification:**\n",
    "   - Identifying categories for text (e.g., spam vs. non-spam emails).\n",
    "\n",
    "2. **Sentiment Analysis:**\n",
    "   - Determining sentiment polarity (positive, negative, neutral).\n",
    "\n",
    "3. **Named Entity Recognition (NER):**\n",
    "   - Extracting entities like names, dates, or locations from text.\n",
    "\n",
    "4. **Machine Translation:**\n",
    "   - Translating text between languages (e.g., Google Translate).\n",
    "\n",
    "5. **Speech-to-Text & Text-to-Speech:**\n",
    "   - Converting spoken words into text and vice versa.\n",
    "\n",
    "6. **Chatbots:**\n",
    "   - Automating human-like responses in customer service systems.\n",
    "\n",
    "### **5. Why is NLP Challenging?**\n",
    "\n",
    "1. **Ambiguity in Language:**\n",
    "   - Words can have multiple meanings depending on context (e.g., \"bank\" could refer to a riverbank or a financial institution).\n",
    "\n",
    "2. **Complex Sentence Structures:**\n",
    "   - Long and nested sentences are difficult for computers to parse.\n",
    "\n",
    "3. **Slang, Idioms, and Variability:**\n",
    "   - Informal language (e.g., tweets, texts) makes consistent interpretation harder.\n",
    "\n",
    "4. **Language Diversity:**\n",
    "   - Processing multiple languages with unique rules is challenging.\n",
    "\n",
    "### **6. NLP Tools and Libraries**\n",
    "\n",
    "1. **NLTK (Natural Language Toolkit):**\n",
    "   - Tokenization, stemming, lemmatization, and basic NLP tasks.\n",
    "\n",
    "2. **spaCy:**\n",
    "   - Fast and efficient for industrial NLP tasks, including NER and dependency parsing.\n",
    "\n",
    "3. **Hugging Face Transformers:**\n",
    "   - Pretrained transformer models like BERT, GPT, and RoBERTa for advanced NLP tasks.\n",
    "\n",
    "4. **TextBlob:**\n",
    "   - Simple library for sentiment analysis and text processing.\n",
    "\n",
    "5. **Gensim:**\n",
    "   - Used for topic modeling and similarity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. NLTK (Natural Language Toolkit)**\n",
    "\n",
    "**NLTK** is one of the most widely used libraries for NLP in Python. It provides tools for preprocessing and analyzing text, including tokenization, stemming, lemmatization, and more.\n",
    "\n",
    "#### **Key Features of NLTK:**\n",
    "1. **Tokenization:**\n",
    "   - Splits text into sentences or words.\n",
    "   - Example: `\"Deep learning is great!\"` → `[\"Deep\", \"learning\", \"is\", \"great\"]`.\n",
    "\n",
    "2. **Stopword Removal:**\n",
    "   - Removes common words like \"is\", \"the\", etc.\n",
    "   - Example: `\"This is an example\"` → `[\"example\"]`.\n",
    "\n",
    "3. **Stemming and Lemmatization:**\n",
    "   - Reduces words to their base or root forms.\n",
    "\n",
    "4. **Part-of-Speech Tagging:**\n",
    "   - Tags words with their grammatical roles (e.g., noun, verb).\n",
    "\n",
    "#### **Example Code:**\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenization Example\n",
    "text = \"Deep learning is amazing. It's used in many applications.\"\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text))\n",
    "print(\"Word Tokenization:\", word_tokenize(text))\n",
    "```\n",
    "\n",
    "### **2. TextBlob**\n",
    "\n",
    "**TextBlob** is a simple-to-use library for text processing. It wraps around NLTK and provides additional features like sentiment analysis, spelling correction, and text translation.\n",
    "\n",
    "#### **Key Features of TextBlob:**\n",
    "1. **Tokenization:**\n",
    "   - Easy tokenization of words and sentences.\n",
    "\n",
    "2. **Spelling Correction:**\n",
    "   - Automatically detects and corrects misspelled words.\n",
    "\n",
    "3. **Text Translation:**\n",
    "   - Translates text between languages using Google Translate API.\n",
    "\n",
    "4. **Sentiment Analysis:**\n",
    "   - Determines the polarity (positive, negative, or neutral) of text.\n",
    "\n",
    "### **3. Comparison of NLTK vs. TextBlob**\n",
    "| **Feature**             | **NLTK**                       | **TextBlob**                     |\n",
    "|--||--|\n",
    "| **Ease of Use**          | Requires more manual setup     | Higher-level and user-friendly   |\n",
    "| **Tokenization**         | Flexible and customizable      | Simplified for quick usage       |\n",
    "| **Spelling Correction**  | Not supported directly         | Built-in (`correct()`)           |\n",
    "| **Translation**          | Not supported directly         | Built-in via Google Translate API|\n",
    "\n",
    "### **4. Key Tasks: Tokenization, Translation, and Spelling**\n",
    "\n",
    "#### **4.1. Tokenization with TextBlob**\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Deep learning is amazing. It's used in many applications.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Sentence and Word Tokenization\n",
    "print(\"Sentence Tokenization:\", blob.sentences)\n",
    "print(\"Word Tokenization:\", blob.words)\n",
    "```\n",
    "\n",
    "#### **4.2. Text Translation with TextBlob**\n",
    "TextBlob uses the Google Translate API for translation.\n",
    "\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Deep learning is amazing.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Translate to Spanish\n",
    "translated = blob.translate(to='es')\n",
    "print(\"Translated Text:\", translated)\n",
    "```\n",
    "\n",
    "#### **4.3. Spelling Correction with TextBlob**\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Deep lerning is amzing.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Correct spelling\n",
    "corrected = blob.correct()\n",
    "print(\"Corrected Text:\", corrected)\n",
    "```\n",
    "\n",
    "### **5. Practical Use Cases**\n",
    "\n",
    "1. **Tokenization:**\n",
    "   - Used in preprocessing pipelines for tasks like classification, translation, and information retrieval.\n",
    "\n",
    "2. **Text Translation:**\n",
    "   - Useful in multilingual NLP tasks such as sentiment analysis, chatbots, and document processing.\n",
    "\n",
    "3. **Spelling Correction:**\n",
    "   - Applied in applications like autocorrect, email filters, and error-prone text input processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentiment Analysis**  \n",
    "**Sentiment Analysis** is the process of determining the **sentiment polarity** of text (positive, negative, neutral) using **Natural Language Processing (NLP)** and **Machine Learning (ML)**. It helps analyze opinions, emotions, and attitudes from unstructured text data.\n",
    "\n",
    "### **1. Methods for Sentiment Analysis**\n",
    "\n",
    "#### **1.1. Rule-Based Approach**\n",
    "- Uses predefined rules or lexicons (e.g., positive and negative word dictionaries).\n",
    "- Tools: **TextBlob**, **VADER (Valence Aware Dictionary and sEntiment Reasoner)**.\n",
    "- **Example:** Words like \"great\" → positive, \"terrible\" → negative.\n",
    "\n",
    "#### **1.2. Machine Learning-Based Approach**\n",
    "- Treats sentiment analysis as a classification problem.\n",
    "- Algorithms: Logistic Regression, SVM, Random Forest, etc.\n",
    "- Features: Bag of Words, TF-IDF, word embeddings.\n",
    "\n",
    "#### **1.3. Deep Learning-Based Approach**\n",
    "- Uses neural networks for more accurate predictions.\n",
    "- Models: LSTM, GRU, CNN, Transformers (BERT, GPT).\n",
    "- Features: Pretrained word embeddings like GloVe, Word2Vec, or context-aware embeddings like BERT.\n",
    "\n",
    "### **2. Preprocessing Techniques for Text Data**\n",
    "\n",
    "#### **2.1. Text Cleaning**\n",
    "- **Lowercasing:** Converts text to lowercase for consistency.\n",
    "- **Remove Punctuation:** Cleans unnecessary symbols.\n",
    "- **Remove Stopwords:** Removes commonly used words that don’t add meaning (e.g., \"is\", \"the\").\n",
    "- **Example:**\n",
    "  - Input: `\"I love programming! It's amazing.\"`\n",
    "  - Output: `\"love programming amazing\"`\n",
    "\n",
    "#### **2.2. Tokenization**\n",
    "- Splits text into sentences or words.\n",
    "- Tools: **NLTK**, **spaCy**, **TextBlob**.\n",
    "- **Example:** `\"Deep learning is fun\"` → `[\"deep\", \"learning\", \"is\", \"fun\"]`\n",
    "\n",
    "#### **2.3. Lemmatization and Stemming**\n",
    "- **Lemmatization:** Converts words to their base form (e.g., \"running\" → \"run\").\n",
    "- **Stemming:** Strips suffixes to reduce words to their root form (e.g., \"happily\" → \"happi\").\n",
    "- Tools: **WordNetLemmatizer (NLTK)**, **spaCy**.\n",
    "\n",
    "#### **2.4. Text Representation**\n",
    "Converts text into numerical format for ML models:\n",
    "1. **Bag of Words (BoW):**\n",
    "   - Represents text as a vector of word counts or binary values.\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "   - Assigns weights based on word frequency and importance.\n",
    "3. **Word Embeddings:**\n",
    "   - Dense vector representations capturing semantic meaning.\n",
    "   - Tools: Word2Vec, GloVe, FastText, or transformer-based embeddings like BERT.\n",
    "\n",
    "### **3. Exploratory Data Analysis (EDA) for Text Data**\n",
    "\n",
    "EDA helps to understand the structure, patterns, and distribution of the data.\n",
    "\n",
    "#### **3.1. Common EDA Techniques**\n",
    "- **Word Frequency Analysis:**\n",
    "  - Identify the most frequent words.\n",
    "  - Tools: **Counter (Python)** or visualization with **WordCloud**.\n",
    "- **Sentence Length Distribution:**\n",
    "  - Plot sentence lengths to understand variability in text.\n",
    "- **Class Distribution:**\n",
    "  - Check the distribution of sentiment classes (e.g., positive vs. negative).\n",
    "- **N-grams Analysis:**\n",
    "  - Analyze word pairs (bigrams) or triplets (trigrams).\n",
    "\n",
    "#### **3.2. Visualization Techniques**\n",
    "1. **WordClouds:**\n",
    "   - Visual representation of word frequency.\n",
    "   ```python\n",
    "   from wordcloud import WordCloud\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   text = \"I love NLP and deep learning. Sentiment analysis is fun!\"\n",
    "   wordcloud = WordCloud().generate(text)\n",
    "   plt.imshow(wordcloud, interpolation='bilinear')\n",
    "   plt.axis(\"off\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "2. **Bar Charts:**\n",
    "   - For visualizing the most frequent words.\n",
    "   ```python\n",
    "   from collections import Counter\n",
    "\n",
    "   words = [\"deep\", \"learning\", \"deep\", \"NLP\", \"fun\", \"fun\", \"learning\"]\n",
    "   word_counts = Counter(words)\n",
    "   word_counts = word_counts.most_common(5)\n",
    "\n",
    "   words, counts = zip(*word_counts)\n",
    "   plt.bar(words, counts)\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "3. **Class Balance:**\n",
    "   - Plot the distribution of sentiment classes using **matplotlib** or **seaborn**.\n",
    "\n",
    "### **4. Workflow for Sentiment Analysis**\n",
    "\n",
    "1. **Data Collection:**\n",
    "   - Gather text data (e.g., tweets, reviews, product feedback).\n",
    "   - Example datasets: IMDB Reviews, Twitter Sentiment Analysis Dataset.\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "   - Apply text cleaning, tokenization, stopword removal, etc.\n",
    "\n",
    "3. **EDA:**\n",
    "   - Visualize and analyze the text to understand patterns.\n",
    "\n",
    "4. **Feature Extraction:**\n",
    "   - Convert text to numerical features using BoW, TF-IDF, or word embeddings.\n",
    "\n",
    "5. **Model Selection:**\n",
    "   - Choose between machine learning (Logistic Regression, SVM) or deep learning (LSTM, BERT).\n",
    "\n",
    "6. **Model Training and Evaluation:**\n",
    "   - Train the model and evaluate it using metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "### **5. Sentiment Analysis Example Using TextBlob**\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text\n",
    "text = \"I absolutely love this product! It's amazing and works perfectly.\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "blob = TextBlob(text)\n",
    "print(\"Sentiment Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Sentiment Subjectivity:\", blob.sentiment.subjectivity)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "Sentiment Polarity: 0.85  # Positive sentiment\n",
    "Sentiment Subjectivity: 0.75  # Highly subjective\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is the foundation of any NLP pipeline, transforming raw, unstructured text into a format suitable for machine learning or deep learning models. Let’s discuss **various methods and techniques for text preprocessing** and how they combine with **Exploratory Data Analysis (EDA)** for deeper insights.\n",
    "\n",
    "### **1. Text Preprocessing Techniques**\n",
    "\n",
    "#### **1.1. Lowercasing**\n",
    "- **Purpose:** Converts all text to lowercase to ensure uniformity.\n",
    "- **Example:**\n",
    "  - Input: `\"Hello World!\"`\n",
    "  - Output: `\"hello world\"`\n",
    "\n",
    "#### **Implementation:**\n",
    "```python\n",
    "text = \"Hello World!\"\n",
    "lowercase_text = text.lower()\n",
    "print(lowercase_text)\n",
    "```\n",
    "\n",
    "#### **1.2. Remove Punctuation**\n",
    "- **Purpose:** Cleans unnecessary symbols like commas, periods, or special characters.\n",
    "- **Example:**\n",
    "  - Input: `\"Hello, World!\"`\n",
    "  - Output: `\"Hello World\"`\n",
    "\n",
    "#### **Implementation:**\n",
    "```python\n",
    "import string\n",
    "\n",
    "text = \"Hello, World!\"\n",
    "no_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(no_punctuation)\n",
    "```\n",
    "\n",
    "#### **1.3. Remove Stopwords**\n",
    "- **Purpose:** Removes common words like \"is\", \"the\", \"a\" that don’t contribute significant meaning.\n",
    "- **Example:**\n",
    "  - Input: `\"This is an example sentence\"`\n",
    "  - Output: `\"example sentence\"`\n",
    "\n",
    "#### **Implementation:**\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"This is an example sentence\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = [word for word in text.split() if word not in stop_words]\n",
    "print(\" \".join(filtered_text))\n",
    "```\n",
    "\n",
    "#### **1.4. Tokenization**\n",
    "- **Purpose:** Splits text into sentences or words.\n",
    "- **Example:**\n",
    "  - Input: `\"Deep learning is amazing!\"`\n",
    "  - Sentence Tokenization: `[\"Deep learning is amazing!\"]`\n",
    "  - Word Tokenization: `[\"Deep\", \"learning\", \"is\", \"amazing\"]`\n",
    "\n",
    "#### **Implementation:**\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Deep learning is amazing. It's used everywhere!\"\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text))\n",
    "print(\"Word Tokenization:\", word_tokenize(text))\n",
    "```\n",
    "\n",
    "#### **1.5. Stemming**\n",
    "- **Purpose:** Reduces words to their root form by removing suffixes.\n",
    "- **Example:**\n",
    "  - Input: `\"running\", \"runner\"`\n",
    "  - Output: `\"run\"`\n",
    "\n",
    "#### **Implementation:**\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runner\", \"runs\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(stemmed_words)\n",
    "```\n",
    "\n",
    "#### **1.6. Lemmatization**\n",
    "- **Purpose:** Converts words to their dictionary base form while preserving meaning.\n",
    "- **Example:**\n",
    "  - Input: `\"better\"`\n",
    "  - Output: `\"good\"`\n",
    "\n",
    "#### **Implementation:**\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"better\", \"running\", \"feet\"]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=\"v\") for word in words]  # \"v\" for verbs\n",
    "print(lemmatized_words)\n",
    "```\n",
    "\n",
    "#### **1.7. Spelling Correction**\n",
    "- **Purpose:** Detects and fixes spelling errors.\n",
    "- **Example:**\n",
    "  - Input: `\"I lovv NLP\"`\n",
    "  - Output: `\"I love NLP\"`\n",
    "\n",
    "#### **Implementation:**\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I lovv NLP\"\n",
    "corrected_text = TextBlob(text).correct()\n",
    "print(corrected_text)\n",
    "```\n",
    "\n",
    "### **2. Exploratory Data Analysis (EDA) for Text Data**\n",
    "\n",
    "EDA provides insights into text data before applying models. Here's how:\n",
    "\n",
    "#### **2.1. Word Frequency Analysis**\n",
    "- Identify the most frequent words in the dataset.\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "text = \"Deep learning is amazing. Deep learning is everywhere.\"\n",
    "words = text.lower().split()\n",
    "word_counts = Counter(words)\n",
    "print(word_counts)\n",
    "```\n",
    "\n",
    "#### **2.2. Sentence Length Analysis**\n",
    "- Analyze sentence lengths to understand the variability in text.\n",
    "\n",
    "```python\n",
    "sentence_lengths = [len(sentence.split()) for sentence in sent_tokenize(text)]\n",
    "print(\"Sentence Lengths:\", sentence_lengths)\n",
    "```\n",
    "\n",
    "#### **2.3. WordCloud Visualization**\n",
    "- Visualize the most frequent words in the dataset.\n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \"Deep learning is amazing and used in many applications.\"\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### **2.4. N-Grams Analysis**\n",
    "- Identify common word pairs (bigrams) or triplets (trigrams).\n",
    "```python\n",
    "from nltk import ngrams\n",
    "\n",
    "text = \"Deep learning is amazing\"\n",
    "bigrams = list(ngrams(text.split(), 2))\n",
    "print(\"Bigrams:\", bigrams)\n",
    "```\n",
    "\n",
    "### **3. Workflow Summary**\n",
    "\n",
    "#### **Preprocessing Pipeline:**\n",
    "1. Lowercase the text.\n",
    "2. Remove punctuation and special characters.\n",
    "3. Remove stopwords.\n",
    "4. Tokenize sentences or words.\n",
    "5. Apply stemming or lemmatization.\n",
    "6. Correct spelling if needed.\n",
    "\n",
    "#### **EDA Pipeline:**\n",
    "1. Analyze word frequencies.\n",
    "2. Visualize text using WordClouds.\n",
    "3. Evaluate sentence/word lengths.\n",
    "4. Analyze N-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What is TextBlob?**\n",
    "\n",
    "**TextBlob** is a simple and easy-to-use Python library for NLP tasks. It is built on top of **NLTK** and **Pattern** and provides a user-friendly interface for text processing.\n",
    "\n",
    "#### **Features of TextBlob:**\n",
    "1. **Sentiment Analysis:**\n",
    "   - Determines the **polarity** (positive/negative/neutral) and **subjectivity** of text.\n",
    "   - Polarity: A value between $-1$ (negative) and $+1$ (positive).\n",
    "   - Subjectivity: A value between $0$ (objective) and $1$ (subjective).\n",
    "   \n",
    "2. **Spelling Correction:**\n",
    "   - Automatically corrects misspelled words.\n",
    "\n",
    "3. **Tokenization:**\n",
    "   - Splits text into words or sentences.\n",
    "\n",
    "4. **Text Translation:**\n",
    "   - Translates text between different languages using the Google Translate API.\n",
    "\n",
    "5. **Lemmatization:**\n",
    "   - Converts words to their dictionary base form.\n",
    "\n",
    "#### **Example Usage:**\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Input text\n",
    "text = \"I absolutly lovv TextBlob. It is awsome for NLP!\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)\n",
    "\n",
    "# Spelling Correction\n",
    "corrected_text = blob.correct()\n",
    "print(\"Corrected Text:\", corrected_text)\n",
    "```\n",
    "\n",
    "### **2. What is RNN (Recurrent Neural Network)?**\n",
    "\n",
    "#### **Definition:**\n",
    "An **RNN** is a type of neural network designed to handle **sequential data**, such as text, time series, or speech. Unlike traditional neural networks, RNNs have a \"memory\" that allows them to process sequences by retaining information from previous steps.\n",
    "\n",
    "#### **Key Characteristics:**\n",
    "1. **Sequential Data Handling:**\n",
    "   - Suitable for tasks where context from previous inputs matters (e.g., predicting the next word in a sentence).\n",
    "\n",
    "2. **Shared Parameters:**\n",
    "   - The same weights are used across all time steps, making the network efficient for sequences of varying lengths.\n",
    "\n",
    "3. **Challenges:**\n",
    "   - **Vanishing Gradient Problem:** Gradients become very small during backpropagation through long sequences, making it hard for the network to learn dependencies over long time intervals.\n",
    "\n",
    "### **3. What is LSTM (Long Short-Term Memory)?**\n",
    "\n",
    "#### **Definition:**\n",
    "**LSTM** is a specialized type of RNN that addresses the **vanishing gradient problem** by introducing **gates** that control the flow of information, enabling it to capture **long-term dependencies** in sequences.\n",
    "\n",
    "#### **Key Components of LSTM:**\n",
    "1. **Forget Gate (\\(f_t\\)):**\n",
    "   - Decides which information to discard from the cell state.\n",
    "   - Formula:\n",
    "     $$\n",
    "     f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "     $$\n",
    "\n",
    "2. **Input Gate (\\(i_t\\)):**\n",
    "   - Decides which new information to add to the cell state.\n",
    "   - Formula:\n",
    "     $$\n",
    "     i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "     $$\n",
    "\n",
    "3. **Cell State (\\(\\tilde{C}_t\\)):**\n",
    "   - Stores long-term memory.\n",
    "   - Formula:\n",
    "     $$\n",
    "     \\tilde{C}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
    "     $$\n",
    "\n",
    "4. **Output Gate (\\(o_t\\)):**\n",
    "   - Determines the output of the LSTM cell.\n",
    "   - Formula:\n",
    "     $$\n",
    "     o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "     $$\n",
    "\n",
    "5. **Final Cell Update (\\(C_t, h_t\\)):**\n",
    "   - Combines forget, input, and output gates to update the hidden and cell states.\n",
    "   - Formula:\n",
    "     $$\n",
    "     C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "     $$\n",
    "     $$\n",
    "     h_t = o_t \\cdot \\tanh(C_t)\n",
    "     $$\n",
    "\n",
    "#### **Advantages of LSTM:**\n",
    "- Can model long-term dependencies in sequential data.\n",
    "- Overcomes the vanishing gradient problem.\n",
    "\n",
    "### **4. Comparison: RNN vs. LSTM**\n",
    "\n",
    "| **Feature**           | **RNN**                                  | **LSTM**                              |\n",
    "||||\n",
    "| **Memory**            | Short-term memory only                  | Long-term memory through gates        |\n",
    "| **Gradient Issues**   | Suffers from vanishing gradient problem  | Solves vanishing gradient problem     |\n",
    "| **Use Cases**         | Simple sequential tasks                 | Tasks requiring long-term context     |\n",
    "\n",
    "### **5. Applications of RNN and LSTM**\n",
    "\n",
    "#### **RNN Applications:**\n",
    "1. **Language Modeling:**\n",
    "   - Predict the next word in a sentence.\n",
    "2. **Time Series Prediction:**\n",
    "   - Predict stock prices or weather patterns.\n",
    "\n",
    "#### **LSTM Applications:**\n",
    "1. **Text Generation:**\n",
    "   - Generate text that mimics the style of the input data.\n",
    "2. **Machine Translation:**\n",
    "   - Translate sentences between languages.\n",
    "3. **Speech Recognition:**\n",
    "   - Convert spoken words to text.\n",
    "\n",
    "### **6. Example: Sentiment Analysis Using LSTM**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example dataset\n",
    "sentences = [\"I love this product\", \"This is terrible\", \"Amazing experience\", \"Not good\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=5)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=64, input_length=5),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_sequences, np.array(labels), epochs=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Word Embeddings**\n",
    "Word embeddings are a type of text representation where **words or phrases are mapped to dense, continuous-valued vectors** in a high-dimensional space. These vectors capture the **semantic meaning** of words by representing them in a way that similar words are close to each other in the vector space.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Why Do We Need Word Embeddings?**\n",
    "Traditional methods like Bag of Words (BoW) or TF-IDF represent text as sparse vectors, where:\n",
    "- Each word gets a unique index.\n",
    "- Vectors are sparse (mostly zeros), and semantic meaning is not captured.\n",
    "\n",
    "**Word embeddings solve these issues:**\n",
    "1. Capture semantic relationships (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\").\n",
    "2. Reduce dimensionality by converting high-dimensional sparse vectors into dense vectors.\n",
    "3. Handle similarity and context effectively (e.g., \"dog\" and \"cat\" are closer than \"dog\" and \"car\").\n",
    "\n",
    "\n",
    "\n",
    "### **3. Key Features of Word Embeddings**\n",
    "- **Dimensionality Reduction:** Typically, words are mapped to vectors of fixed size, such as 50, 100, or 300 dimensions.\n",
    "- **Semantic Proximity:** Similar words have similar vectors in the embedding space.\n",
    "- **Context Awareness:** Some embeddings (like Word2Vec) capture the context in which words appear.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Techniques to Create Word Embeddings**\n",
    "\n",
    "#### **4.1. Word2Vec**\n",
    "- **Approach:** Predicts context words given a target word (CBOW) or predicts a target word given context words (Skip-Gram).\n",
    "- **Advantages:**\n",
    "  - Captures semantic and syntactic relationships.\n",
    "- **Example:**\n",
    "  - The model learns that \"king\" - \"man\" + \"woman\" ≈ \"queen\".\n",
    "\n",
    "\n",
    "\n",
    "#### **4.2. GloVe (Global Vectors for Word Representation)**\n",
    "- **Approach:** Captures relationships between words by analyzing the co-occurrence matrix.\n",
    "- **Advantages:**\n",
    "  - Combines local context (Word2Vec) with global context.\n",
    "\n",
    "\n",
    "\n",
    "#### **4.3. FastText**\n",
    "- **Approach:** Treats words as subword components (n-grams).\n",
    "- **Advantages:**\n",
    "  - Handles out-of-vocabulary (OOV) words by understanding subword information.\n",
    "\n",
    "\n",
    "\n",
    "#### **4.4. Contextual Embeddings (BERT, GPT, etc.)**\n",
    "- **Approach:** Uses transformers to generate embeddings based on the context of a word in a sentence.\n",
    "- **Advantages:**\n",
    "  - Different embeddings for the same word in different contexts.\n",
    "  - Example: \"bank\" in \"river bank\" vs. \"money bank\" has different embeddings.\n",
    "\n",
    "\n",
    "\n",
    "### **5. Vector Representation in Word Embeddings**\n",
    "\n",
    "#### **Dense Vectors:**\n",
    "Each word is represented as a dense vector of fixed dimensions.\n",
    "\n",
    "#### **Example:**\n",
    "Word embedding of size 3 for \"king\", \"queen\", and \"man\":\n",
    "$$\n",
    "\\text{king} = [0.8, 0.6, 0.3]\n",
    "$$\n",
    "$$\n",
    "\\text{queen} = [0.9, 0.7, 0.4]\n",
    "$$\n",
    "$$\n",
    "\\text{man} = [0.7, 0.4, 0.1]\n",
    "$$\n",
    "\n",
    "#### **Semantic Relationships:**\n",
    "- \\( \\text{king} - \\text{man} + \\text{woman} = \\text{queen} \\)\n",
    "\n",
    "\n",
    "\n",
    "### **6. Word Embeddings in Action**\n",
    "\n",
    "#### **6.1. Cosine Similarity**\n",
    "Measures how similar two word vectors are:\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
    "$$\n",
    "\n",
    "#### **6.2. Applications**\n",
    "1. **Text Similarity:** Finding similarity between sentences or words.\n",
    "2. **Document Classification:** Classify documents based on embedding representations.\n",
    "3. **Named Entity Recognition (NER):** Identify entities like names, dates, etc.\n",
    "4. **Machine Translation:** Translate words/sentences using embeddings.\n",
    "\n",
    "### **7. Pretrained Word Embedding Models**\n",
    "\n",
    "#### **7.1. Word2Vec**\n",
    "- Available from **Google's pretrained embeddings**.\n",
    "\n",
    "#### **7.2. GloVe**\n",
    "- Pretrained embeddings available from Stanford NLP.\n",
    "\n",
    "#### **7.3. FastText**\n",
    "- Facebook AI’s library for subword-based embeddings.\n",
    "\n",
    "#### **7.4. Transformer-based Models**\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers):**\n",
    "  Contextual embeddings.\n",
    "- **GPT (Generative Pretrained Transformer):**\n",
    "  Specialized for text generation.\n",
    "\n",
    "### **8. Example: Using Pretrained Word Embeddings**\n",
    "\n",
    "#### **Using GloVe Pretrained Embeddings**\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pretrained GloVe embeddings\n",
    "embeddings = KeyedVectors.load_word2vec_format('glove.6B.300d.txt', binary=False)\n",
    "\n",
    "# Get word vector for \"king\"\n",
    "king_vector = embeddings['king']\n",
    "\n",
    "# Find most similar words\n",
    "similar_words = embeddings.most_similar('king')\n",
    "print(similar_words)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec: An Overview**\n",
    "\n",
    "**Word2Vec** is a powerful word embedding technique developed by Google to represent words as dense vectors of fixed size, capturing semantic relationships between words. It is based on the idea that words appearing in similar contexts tend to have similar meanings.\n",
    "\n",
    "\n",
    "\n",
    "### **1. Core Concept**\n",
    "Word2Vec converts words into **dense vector representations** where:\n",
    "- Words with similar meanings are close in the vector space.\n",
    "- It uses **contextual information** to determine relationships.\n",
    "\n",
    "#### **Famous Principle:**\n",
    "*\"You shall know a word by the company it keeps.\"*  \n",
    "This is based on the **distributional hypothesis** in linguistics.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Architecture of Word2Vec**\n",
    "\n",
    "#### Word2Vec has two main architectures:\n",
    "1. **CBOW (Continuous Bag of Words):**\n",
    "   - Predicts the **target word** based on its **context words**.\n",
    "   - Example: In \"The cat sat on the ___,\" it predicts the missing word \"mat.\"\n",
    "   - Efficient for smaller datasets.\n",
    "\n",
    "   **Diagram:**\n",
    "   $$\n",
    "   \\text{Input (context words)} \\rightarrow \\text{Hidden Layer} \\rightarrow \\text{Output (target word)}\n",
    "   $$\n",
    "\n",
    "2. **Skip-Gram:**\n",
    "   - Predicts **context words** given the **target word**.\n",
    "   - Example: Given the word \"cat,\" it predicts surrounding words like \"The,\" \"sat,\" and \"on.\"\n",
    "   - Works better with larger datasets and rare words.\n",
    "\n",
    "   **Diagram:**\n",
    "   $$\n",
    "   \\text{Input (target word)} \\rightarrow \\text{Hidden Layer} \\rightarrow \\text{Output (context words)}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "### **3. Mathematical Representation**\n",
    "\n",
    "#### **Skip-Gram Objective:**\n",
    "Maximizes the probability of context words \\( w_{context} \\) given a target word \\( w_{target} \\).\n",
    "\n",
    "$$\n",
    "P(w_{context} \\mid w_{target}) = \\frac{\\exp(v_{context} \\cdot v_{target})}{\\sum_{w'} \\exp(v_{w'} \\cdot v_{target})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( v_{context} \\): Vector of the context word.\n",
    "- \\( v_{target} \\): Vector of the target word.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Training Word2Vec**\n",
    "The training process involves:\n",
    "1. **Input Layer:** One-hot encoding of the target word.\n",
    "2. **Hidden Layer:** Projects the input into a dense vector representation.\n",
    "3. **Output Layer:** Predicts probabilities of context words.\n",
    "\n",
    "#### **Optimization Techniques:**\n",
    "- **Negative Sampling:** Simplifies computation by focusing on a subset of negative examples.\n",
    "- **Hierarchical Softmax:** Reduces the computational cost of softmax over large vocabularies.\n",
    "\n",
    "\n",
    "\n",
    "### **5. Advantages of Word2Vec**\n",
    "1. **Semantic Relationships:**\n",
    "   - Captures relationships like **vector arithmetic**:\n",
    "     $$\n",
    "     \\text{King} - \\text{Man} + \\text{Woman} \\approx \\text{Queen}\n",
    "     $$\n",
    "\n",
    "2. **Efficient Representations:**\n",
    "   - Dense vectors capture more information than sparse representations like Bag of Words.\n",
    "\n",
    "3. **Context Sensitivity:**\n",
    "   - Context words influence embeddings, enabling better understanding of semantics.\n",
    "\n",
    "\n",
    "\n",
    "### **6. Example: Using Word2Vec in Python**\n",
    "\n",
    "#### **6.1. Training Word2Vec Using Gensim**\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the rug\",\n",
    "    \"Cats and dogs are friends\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Word embedding for 'cat'\n",
    "print(\"Embedding for 'cat':\", model.wv['cat'])\n",
    "\n",
    "# Similar words to 'cat'\n",
    "print(\"Words similar to 'cat':\", model.wv.most_similar('cat'))\n",
    "```\n",
    "\n",
    "#### **6.2. Using Pretrained Word2Vec (Google News)**\n",
    "```python\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pretrained Word2Vec model\n",
    "pretrained_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Word embedding for 'king'\n",
    "print(\"Embedding for 'king':\", pretrained_model['king'])\n",
    "\n",
    "# Word analogy: King - Man + Woman = Queen\n",
    "print(\"Result of analogy (King - Man + Woman):\", pretrained_model.most_similar(positive=['king', 'woman'], negative=['man']))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **7. Limitations of Word2Vec**\n",
    "1. **Lacks Contextual Awareness:**\n",
    "   - The embedding of a word like \"bank\" remains the same, whether it means \"riverbank\" or \"money bank.\"\n",
    "   - This was later addressed by **contextual embeddings** (e.g., BERT).\n",
    "\n",
    "2. **Data Dependency:**\n",
    "   - Requires large amounts of training data to capture meaningful relationships.\n",
    "\n",
    "3. **Static Vocabulary:**\n",
    "   - Cannot handle out-of-vocabulary (OOV) words.\n",
    "\n",
    "\n",
    "\n",
    "### **8. Applications of Word2Vec**\n",
    "1. **Sentiment Analysis:** Analyzing the polarity of sentences.\n",
    "2. **Document Similarity:** Finding similarity between documents.\n",
    "3. **Recommendation Systems:** Suggesting related items based on semantic similarity.\n",
    "4. **Machine Translation:** Mapping words between languages using embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in **Continuous Bag of Words (CBOW)** are as follows:\n",
    "\n",
    "1. **Context and Target Selection**: \n",
    "   - For a given word in a sentence (target word), choose a window of context words around it. The context consists of words surrounding the target word within a defined window size.\n",
    "   \n",
    "2. **One-Hot Encoding**: \n",
    "   - Each word in the context is represented as a one-hot vector, where the vector corresponds to the vocabulary size and has a 1 at the index of the word, with 0s elsewhere.\n",
    "   \n",
    "3. **Embedding Layer**: \n",
    "   - One-hot encoded vectors are mapped to dense vectors (word embeddings), each representing a word in the vocabulary.\n",
    "\n",
    "4. **Context Vector Calculation**: \n",
    "   - The context vectors are averaged to form a single context vector.\n",
    "\n",
    "5. **Prediction**: \n",
    "   - The context vector is passed through a neural network to predict the target word.\n",
    "\n",
    "6. **Optimization**: \n",
    "   - The model is trained by adjusting the weights to minimize the difference between the predicted target word and the actual target word using a loss function like Cross-Entropy.\n",
    "\n",
    "### **Window Size in CBOW**:\n",
    "- **Window size** refers to the number of context words surrounding a target word. For example, with a window size of 2, you would have two words to the left and two words to the right of the target word as context. A larger window captures more context but might result in less specific representations, while a smaller window captures more local context but might miss broader meanings.\n",
    "\n",
    "### **One-Hot Encoding**:\n",
    "- **One-hot encoding** is a method of representing words as binary vectors. For a vocabulary of size \\(V\\), each word is represented by a vector of length \\(V\\) with a 1 at the position corresponding to the word and 0s elsewhere. \n",
    "- For example, in a vocabulary of three words (cat, dog, bird), \"cat\" might be represented as \\([1, 0, 0]\\), \"dog\" as \\([0, 1, 0]\\), and \"bird\" as \\([0, 0, 1]\\).\n",
    "\n",
    "### **Drawbacks of One-Hot Encoding**:\n",
    "1. **High Dimensionality**:\n",
    "   - One-hot vectors can be extremely sparse and high-dimensional, especially for large vocabularies, which can be inefficient in terms of memory and computation.\n",
    "   \n",
    "2. **No Semantic Relationship**:\n",
    "   - One-hot encoding doesn’t capture any semantic similarity between words. For example, \"dog\" and \"cat\" would be represented as completely different vectors even though they share some semantic meaning.\n",
    "   \n",
    "3. **Scalability Issues**:\n",
    "   - As vocabulary size grows, the one-hot vectors become larger, leading to scalability issues. Handling large vocabularies can be computationally expensive.\n",
    "\n",
    "4. **Lack of Contextual Information**:\n",
    "   - One-hot encoding doesn't provide any information about the context or the relationships between words in a sentence or corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nature of a **text analysis problem**—whether it is a **regression** or **classification** problem—depends on the task being performed. Here's how to differentiate:\n",
    "\n",
    "### **Classification** Problem:\n",
    "Text analysis is often a **classification** problem when you are categorizing text into predefined categories or classes. Examples include:\n",
    "- **Sentiment Analysis**: Classifying text as positive, negative, or neutral.\n",
    "- **Spam Detection**: Classifying emails or messages as spam or not spam.\n",
    "- **Topic Categorization**: Assigning articles to categories such as sports, politics, technology, etc.\n",
    "\n",
    "In these cases, the model's goal is to predict a **discrete** label from a set of possible classes, making it a classification problem.\n",
    "\n",
    "### **Regression** Problem:\n",
    "Text analysis can be a **regression** problem when the goal is to predict a continuous value from text. Examples include:\n",
    "- **Rating Prediction**: Predicting the star rating (e.g., from 1 to 5 stars) for a review based on the text content.\n",
    "- **Sentiment Score**: Predicting a numerical sentiment score, like the intensity of sentiment on a scale of -1 to 1.\n",
    "\n",
    "In these cases, the output is a **continuous value**, making it a regression problem.\n",
    "\n",
    "### In Summary:\n",
    "- **Classification**: When you're predicting discrete categories or labels (e.g., positive/negative sentiment).\n",
    "- **Regression**: When you're predicting continuous values (e.g., rating prediction)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
