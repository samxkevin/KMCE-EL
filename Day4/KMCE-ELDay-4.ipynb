{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Machine Learning as Function Approximation**\n",
    "Machine Learning models approximate a mathematical function, $f(x)$, that maps inputs to outputs:\n",
    "$$\n",
    "y = f(x; \\theta)\n",
    "$$\n",
    "Where:\n",
    "- $x$: Input features.\n",
    "- $\\theta$: Learnable parameters (weights, biases).\n",
    "- $y$: Predicted output.\n",
    "\n",
    "The goal is to learn the optimal $\\theta$ by minimizing the **loss function**, which measures the error between predictions and true outputs.\n",
    "\n",
    "#### **Single Perceptron**\n",
    "A **single perceptron** models linear relationships:\n",
    "$$\n",
    "y = \\sigma(w \\cdot x + b)\n",
    "$$\n",
    "- **Limitation:** Only capable of solving **linearly separable problems**, making it unsuitable for real-world tasks with **non-linearity and noise**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Deep Neural Networks (DNNs)**\n",
    "A **Deep Neural Network (DNN)** has multiple layers of neurons, enabling it to model complex, non-linear relationships in data:\n",
    "- **Input Layer:** Accepts raw data.\n",
    "- **Hidden Layers:** Perform transformations to learn intermediate representations.\n",
    "- **Output Layer:** Produces predictions.\n",
    "\n",
    "#### **Fully Connected Layers (Dense Layers)**\n",
    "- _Each neuron in one layer is connected to every neuron in the next layer_ .\n",
    "- The number of **weights** in a layer is:\n",
    "$$\n",
    "\\text{Number of Weights} = (\\text{Number of Inputs}) \\times (\\text{Number of Neurons})\n",
    "$$\n",
    "- **Transposing Weights:** In some architectures, weights are transposed to align their dimensions with matrix multiplication during forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Activation Functions**\n",
    "We use activation function to capture multiple linear functions in our data. Activation function which are non linear are necessary in this case. We are trying to learn the relationship between input and output in the Data.\n",
    "\n",
    "\n",
    "- **Purpose:** They determine whether a neuron should activate or not by transforming the weighted sum of inputs.\n",
    "- **Key Role:** Introduce **non-linearity**, allowing neural networks to model complex patterns in data. Without activation functions, a neural network would behave like a linear model regardless of the number of layers.\n",
    "\n",
    "### **a. Sigmoid Activation Function**\n",
    "\n",
    "#### **Definition:**\n",
    "The **sigmoid function** maps input values to a range between $0$ and $1$.  \n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### **Properties:**\n",
    "- Output: $(0, 1)$.\n",
    "- Differentiable everywhere.\n",
    "- Symmetric around $x = 0.5$.\n",
    "\n",
    "#### **Use Cases:**\n",
    "- Historically used in the **output layer** for **binary classification** tasks.\n",
    "- Models probabilities, as outputs lie between $0$ and $1$.\n",
    "\n",
    "#### **Advantages:**\n",
    "1. Maps large inputs into a small range, making it interpretable for probability-based tasks.\n",
    "2. Smooth and differentiable, enabling gradient-based optimization.\n",
    "\n",
    "#### **Drawbacks:**\n",
    "1. **Vanishing Gradient Problem:**\n",
    "   - Gradients diminish as $x$ becomes very large or very small.\n",
    "   - Derivative:\n",
    "     $$\n",
    "     \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "     $$\n",
    "     At extreme $x$, $\\sigma(x)$ saturates near $0$ or $1$, leading to very small gradients.\n",
    "\n",
    "2. **Not Zero-Centric:**\n",
    "   - Outputs are always positive, causing gradients to have consistent sign across layers, which slows convergence.\n",
    "\n",
    "3. **Exponential Computation:**\n",
    "   - Relatively expensive due to $e^{-x}$.\n",
    "\n",
    "\n",
    "\n",
    "### **b. Tanh (Hyperbolic Tangent) Activation Function**\n",
    "\n",
    "#### **Definition:**\n",
    "The **Tanh** function maps inputs to a range between $-1$ and $1$.  \n",
    "$$\n",
    "\\text{Tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "#### **Properties:**\n",
    "- Output: $(-1, 1)$.\n",
    "- Differentiable everywhere.\n",
    "- Symmetric around the origin ($0$).\n",
    "\n",
    "#### **Use Cases:**\n",
    "- Commonly used in hidden layers of neural networks.\n",
    "- Zero-centered output helps with balanced gradient updates.\n",
    "\n",
    "#### **Advantages:**\n",
    "1. **Zero-Centric Output:** Improves optimization as gradients are balanced (positive and negative).\n",
    "2. **Smooth Non-Linearity:** Suitable for tasks requiring subtle activations.\n",
    "\n",
    "#### **Drawbacks:**\n",
    "1. **Vanishing Gradient Problem:** Similar to Sigmoid, gradients shrink for extreme $x$ values.\n",
    "2. **Computational Cost:** Requires exponential computations, making it slower than simpler functions.\n",
    "\n",
    "\n",
    "\n",
    "### **b. ReLU (Rectified Linear Unit) Activation Function**\n",
    "\n",
    "#### **Definition:**\n",
    "The **ReLU function** outputs the input if it’s positive, and $0$ otherwise.  \n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "#### **Properties:**\n",
    "- Output: $[0, \\infty)$.\n",
    "- Differentiable for $x > 0$.\n",
    "\n",
    "#### **Use Cases:**\n",
    "- Most commonly used in **hidden layers** of modern neural networks.\n",
    "- Preferred for deep networks due to computational simplicity.\n",
    "\n",
    "#### **Advantages:**\n",
    "1. **Efficient Computation:** No expensive operations (e.g., exponentials).\n",
    "2. **Non-Saturating Gradients:** Unlike Sigmoid and Tanh, ReLU doesn’t saturate for large positive values.\n",
    "3. **Sparse Activation:** Activates only a fraction of neurons, improving efficiency.\n",
    "\n",
    "#### **Drawbacks:**\n",
    "1. **Dying ReLU Problem:** Neurons with negative inputs always output $0$, and their gradients become $0$, effectively killing those neurons.\n",
    "2. **Unbounded Output:** Large values can destabilize optimization.\n",
    "\n",
    "\n",
    "\n",
    "### **c. Leaky ReLU**\n",
    "\n",
    "#### **Definition:**\n",
    "A variation of ReLU that introduces a small slope for negative inputs:  \n",
    "$$\n",
    "\\text{Leaky ReLU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Where $\\alpha$ is a small positive constant (e.g., $0.01$).\n",
    "\n",
    "#### **Advantages:**\n",
    "1. **Solves Dying ReLU Problem:** Negative inputs produce small but non-zero outputs, keeping neurons alive.\n",
    "2. **Efficient Computation:** Similar to ReLU.\n",
    "\n",
    "#### **Drawbacks:**\n",
    "1. The choice of $\\alpha$ is a hyperparameter requiring tuning.\n",
    "\n",
    "\n",
    "\n",
    "### **d. Softmax Activation Function**\n",
    "\n",
    "#### **Definition:**\n",
    "Used in **multi-class classification**, it converts logits into probabilities:  \n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "#### **Properties:**\n",
    "- Output: $(0, 1)$ for each class, with all probabilities summing to $1$.\n",
    "\n",
    "#### **Use Cases:**\n",
    "- **Output layer** for multi-class classification.\n",
    "\n",
    "#### **Advantages:**\n",
    "1. Outputs probabilities, making results interpretable.\n",
    "2. Normalizes outputs, ensuring the total equals $1$.\n",
    "\n",
    "#### **Drawbacks:**\n",
    "1. Computationally expensive due to the exponentials and summation.\n",
    "2. **Vanishing Gradient Problem** can occur for very large logits.\n",
    "\n",
    "\n",
    "\n",
    "### **e. Other Notable Activation Functions**\n",
    "1. **Swish:**  \n",
    "   $$\n",
    "   \\text{Swish}(x) = x \\cdot \\sigma(x)\n",
    "   $$  \n",
    "   Combines the properties of Sigmoid and linear functions. Smooth and non-saturating.\n",
    "\n",
    "2. **GELU (Gaussian Error Linear Unit):**  \n",
    "   Smoothly combines linear and non-linear behavior. Often used in transformer architectures like **BERT**.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| **Activation**   | **Range**            | **Zero-Centric** | **Pros**                                      | **Cons**                                        |\n",
    "|------------------|----------------------|------------------|-----------------------------------------------|-------------------------------------------------|\n",
    "| **Sigmoid**      | $(0, 1)$             | ❌                | Probabilistic output, smooth gradients        | Vanishing gradients, not zero-centric           |\n",
    "| **Tanh**         | $(-1, 1)$            | ✅                | Zero-centered, good for hidden layers         | Vanishing gradients, computationally costly     |\n",
    "| **ReLU**         | $[0, \\infty)$        | ❌                | Efficient, non-saturating, sparse activations  | Dying ReLU problem, unbounded output            |\n",
    "| **Leaky ReLU**   | $(-\\infty, \\infty)$   | ✅                | Solves Dying ReLU, efficient                  | Requires tuning of $\\alpha$                     |\n",
    "| **Softmax**      | $(0, 1)$             | ❌                | Probability distribution for multi-class       | Computationally expensive                       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Object-Oriented Design in Neural Networks**\n",
    "It is always better to compute object oriented because it is always extensible and does not require changing the functional code. Linear problems are given to the Activation Function.Which are dense in nature. We put the common functionality to Base class which shall be implemented for all the core. \n",
    "\n",
    "Building neural networks using **object-oriented programming (OOP)** ensures:\n",
    "- **Reusability:** Common functionality (e.g., layers, activations) is defined in base classes.\n",
    "- **Extensibility:** New layers or methods can be added without modifying existing code.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "class BaseLayer:\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "class DenseLayer(BaseLayer):\n",
    "    def forward(self, input):\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Backpropagation**\n",
    "**Backpropagation** computes gradients of the loss with respect to each parameter using the **chain rule**.  We do backpropagation for finding the loss at node at last. Error is propagated from the back side. Error is calculated at the Later Nodes and is propagated back.\n",
    "- **Forward Pass:** Calculate predictions and loss.\n",
    "- **Backward Pass:** Propagate errors from the output layer back to earlier layers, adjusting weights:\n",
    "  $$\n",
    "  w \\leftarrow w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "  $$\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "### __Important Notes:__\n",
    "In a **forward pass**, the data flows from the input layer to the output layer, making predictions at the end. However, if the output is wrong, we can't directly know which layers contributed to the error, especially if the network has many layers.\n",
    "\n",
    "#### __Intuitive Explanation:__\n",
    "Imagine you’re trying to solve a complex math problem, and your final answer is wrong. Even though your calculations along the way were correct, you can’t tell exactly where the mistake happened just by looking at the final answer. You need a way to **backtrack** and figure out where things went wrong in the middle steps.\n",
    "\n",
    "This is where **backpropagation** comes in. After the forward pass, backpropagation helps you work backward through the layers to identify which ones contributed to the error and by how much. This way, even though the layers in the front may have produced correct outputs, backpropagation ensures the **whole network** learns and adjusts based on the final error.\n",
    "\n",
    "In short, **forward pass** is great for making predictions, but we need **backpropagation** to fix mistakes, especially in deep networks, by adjusting each layer's contribution based on the error.\n",
    "\n",
    "### **Why Is Backpropagation Necessary?**\n",
    "\n",
    "1. **Optimizing Parameters:**\n",
    "   - Neural networks rely on **weights** and **biases** to map inputs to outputs.\n",
    "   - Backpropagation calculates the **gradients** of the loss function with respect to each parameter.\n",
    "   - These gradients guide the optimizer (e.g., gradient descent) to update parameters to minimize the loss:\n",
    "     $$\n",
    "     w \\leftarrow w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "     $$\n",
    "     Where:\n",
    "     - $w$: Weight\n",
    "     - $\\eta$: Learning rate\n",
    "     - $\\frac{\\partial L}{\\partial w}$: Gradient of the loss with respect to the weight\n",
    "\n",
    "\n",
    "\n",
    "2. **Handling Complex Relationships:**\n",
    "   - In real-world problems, input-output relationships are often **non-linear** and complex.\n",
    "   - Backpropagation works layer-by-layer, ensuring each weight is adjusted based on its specific contribution to the error, even in deep networks with multiple layers.\n",
    "\n",
    "\n",
    "\n",
    "3. **Computational Efficiency:**\n",
    "   - Backpropagation efficiently uses the **chain rule** to propagate errors from the output layer to earlier layers.\n",
    "   - Without it, calculating gradients manually for each parameter in large networks would be computationally infeasible.\n",
    "\n",
    "\n",
    "\n",
    "### **Where Is Backpropagation Necessary?**\n",
    "\n",
    "#### **1. Supervised Learning:**\n",
    "- **When:** Tasks like classification and regression.\n",
    "- **Why:** To reduce the error between the predicted labels and ground truth using a loss function (e.g., Mean Squared Error, Cross-Entropy Loss).\n",
    "- **Example:**\n",
    "  - In image classification, backpropagation updates weights to improve the model's accuracy in identifying objects.\n",
    "\n",
    "#### **2. Deep Neural Networks (DNNs):**\n",
    "- **When:** For architectures with multiple layers (e.g., convolutional networks, recurrent networks).\n",
    "- **Why:** Each layer depends on the outputs of previous layers, making backpropagation crucial to calculate gradients for every layer.\n",
    "\n",
    "#### **3. Fine-Tuning Pretrained Models:**\n",
    "- **When:** Transfer learning with pretrained models like BERT or ResNet.\n",
    "- **Why:** Backpropagation fine-tunes weights of the pretrained layers for the new task.\n",
    "\n",
    "#### **4. Reinforcement Learning (Certain Cases):**\n",
    "- **When:** Training policies in neural networks with differentiable components.\n",
    "- **Why:** Backpropagation helps calculate gradients for value or policy functions.\n",
    "\n",
    "\n",
    "\n",
    "### **Why Can't We Skip Backpropagation?**\n",
    "\n",
    "- Without backpropagation, **parameter updates** would be random or incorrect, leading to:\n",
    "  - Poor training performance.\n",
    "  - Non-convergence of the model.\n",
    "- Real-world problems like language translation or image recognition require structured gradient updates to learn efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Oher Gradient Descent Algorithms:__\n",
    "\n",
    "### **Momentum in Gradient Descent**:\n",
    "Momentum helps accelerate the gradient descent process and avoids getting stuck in saddle points (flat regions where gradients are close to zero).\n",
    "\n",
    "- **How it Works**:\n",
    "  - Momentum keeps track of past gradients to adjust the current gradient update. This means if the gradients have been consistently pointing in the same direction, the algorithm will continue in that direction, making updates faster.\n",
    "  - The formula for momentum is:\n",
    "    $$\n",
    "    v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla L\n",
    "    $$\n",
    "    Here, \\( v_t \\) is the velocity (running average of the gradients), and \\( \\beta \\) is the momentum term (usually close to 1).\n",
    "    - The weight update formula is:\n",
    "    $$\n",
    "    w \\leftarrow w - \\eta v_t\n",
    "    $$\n",
    "    where \\( \\eta \\) is the learning rate and \\( v_t \\) is the adjusted gradient.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Helps escape saddle points by smoothing the updates and reducing oscillations.\n",
    "  - Accelerates the gradient descent in the direction of the gradient, making the process faster and more stable.\n",
    "\n",
    "- **Why use it?**:\n",
    "  - It's useful when the gradient is noisy, as it smooths out fluctuations, allowing the model to converge faster and avoid getting stuck in local minima or saddle points.\n",
    "\n",
    "\n",
    "\n",
    "### **Adagrad**:\n",
    "Adagrad adjusts the learning rate for each parameter based on the accumulated squared gradients, allowing the algorithm to adaptively scale the learning rate for each parameter.\n",
    "\n",
    "- **How it Works**:\n",
    "  - The formula for Adagrad's update is:\n",
    "    $$\n",
    "    w \\leftarrow w - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla L\n",
    "    $$\n",
    "    where \\( G_t \\) is the cumulative sum of squared gradients, and \\( \\epsilon \\) is a small constant to prevent division by zero.\n",
    "  \n",
    "- **Advantages**:\n",
    "  - Adagrad is great for dealing with **sparse data**, where some features appear infrequently (e.g., in NLP or text classification tasks). It gives larger updates to infrequent features and smaller updates to frequent ones.\n",
    "  \n",
    "- **Why use it?**:\n",
    "  - If the dataset is sparse (some features are very infrequent), Adagrad will give larger learning rates to these sparse features, which can be useful for better generalization. It’s also ideal for problems like natural language processing (NLP) where feature sparsity is common.\n",
    "\n",
    "- **Downside**:\n",
    "  - The learning rate shrinks too quickly over time, which can lead to stagnation during the later stages of training.\n",
    "\n",
    "\n",
    "\n",
    "### **RMSProp**:\n",
    "RMSProp (Root Mean Squared Propagation) improves Adagrad by introducing an **exponential moving average** of squared gradients. This prevents the learning rate from decaying too rapidly as in Adagrad.\n",
    "\n",
    "- **How it Works**:\n",
    "  - RMSProp modifies Adagrad by considering the recent gradients rather than all historical gradients. The formula for RMSProp is:\n",
    "    $$\n",
    "    E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2\n",
    "    $$\n",
    "    where \\( E[g^2]_t \\) is the moving average of squared gradients, and \\( \\beta \\) is the decay factor (typically set to 0.9).\n",
    "    - The weight update formula is:\n",
    "    $$\n",
    "    w \\leftarrow w - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla L\n",
    "    $$\n",
    "  \n",
    "- **Advantages**:\n",
    "  - **Prevents learning rate decay**: By using an exponentially decaying average of squared gradients, RMSProp avoids the rapid decay problem of Adagrad, making it more suitable for long-term training.\n",
    "  - **Effective for non-stationary objectives**: It works well for tasks like training recurrent neural networks (RNNs) or deep networks.\n",
    "\n",
    "- **Why use it?**:\n",
    "  - It’s better than Adagrad because it keeps the learning rate from shrinking too quickly. This makes it more stable and effective for a wider range of problems, especially in non-stationary settings where the data distribution may change over time.\n",
    "\n",
    "\n",
    "\n",
    "### **Comparing Adagrad, Momentum, and RMSProp**:\n",
    "\n",
    "1. **Adagrad vs Momentum**:\n",
    "   - **Adagrad** is suited for problems with sparse data, where some parameters (features) are updated much less frequently. The learning rate decreases over time, which helps reduce the impact of frequent updates.\n",
    "   - **Momentum**, on the other hand, doesn't adjust the learning rate for individual parameters but rather accelerates the gradient descent process by maintaining a velocity term. It is better for smooth convergence, especially when there are oscillations or noise in the gradients.\n",
    "   \n",
    "   **Why Adagrad can be better than Momentum**:\n",
    "   - In tasks like NLP where features are sparse, **Adagrad** is more efficient because it adapts the learning rate to each parameter individually. In contrast, **Momentum** doesn’t focus on sparse data and might not handle varying learning rates as effectively.\n",
    "\n",
    "2. **RMSProp vs Adagrad**:\n",
    "   - **RMSProp** is an improvement over **Adagrad**. The key difference is that RMSProp introduces an exponentially weighted average of the squared gradients, so the learning rate doesn't decay too fast. This makes it more effective in long-term training scenarios.\n",
    "   - **Adagrad** is good for sparse data but suffers from rapidly shrinking learning rates. **RMSProp** overcomes this issue and can be more efficient in scenarios where you need a more stable learning rate over time.\n",
    "\n",
    "   **Why RMSProp is better than Adagrad**:\n",
    "   - **RMSProp** is more stable than **Adagrad**, especially for non-stationary problems. The use of a decaying average of gradients helps prevent the learning rate from becoming too small over time, making the optimizer more reliable in the later stages of training.\n",
    "\n",
    "### **Conclusion**:\n",
    "- Use **Momentum** when you need smoother, faster convergence and to escape saddle points.\n",
    "- Use **Adagrad** if you have sparse data where each feature’s importance varies, but be aware of the rapid learning rate decay.\n",
    "- Use **RMSProp** if you want the benefits of Adagrad but with a more stable and effective learning rate over time, especially in non-stationary scenarios."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
