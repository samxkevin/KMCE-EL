{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Moving Towards PyTorch Now:__\n",
    "### **Device Handling in PyTorch**\n",
    "In PyTorch, computations can be performed on either the **CPU** or the **GPU**. To leverage the GPU (if available), we use the following code stub:\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "```\n",
    "- **Why?** GPU computations are much faster for deep learning tasks due to their parallel processing capabilities.\n",
    "- **Switching Tensors Between CPU and GPU:** We move tensors using `.to()`:\n",
    "  ```python\n",
    "  tensor = tensor.to(device)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Indexing and Slicing in Tensors**\n",
    "1. **Basic Indexing:** Access elements using row and column indices:\n",
    "   ```python\n",
    "   x[row_index, column_index]\n",
    "   ```\n",
    "   For a tensor $x$, `x[0, 1]` fetches the element at the first row and second column.\n",
    "\n",
    "2. **Slicing:** Extract multiple rows/columns using ranges:\n",
    "   ```python\n",
    "   x[row_start:row_end, col_start:col_end]\n",
    "   ```\n",
    "   Example: `x[:, 0]` fetches all rows of the first column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Autograd**\n",
    "The **Autograd** module in PyTorch automatically computes gradients during backpropagation.  \n",
    "- **Core Idea:** It builds a **computation graph** as operations are performed, enabling the calculation of derivatives for all parameters involved.\n",
    "- **Key Features:**\n",
    "  - `requires_grad=True`: Tracks operations for gradient computation.\n",
    "    ```python\n",
    "    x = torch.tensor(3.0, requires_grad=True)\n",
    "    ```\n",
    "  - `.backward()`: Computes gradients for tensors in the computation graph.\n",
    "    ```python\n",
    "    loss.backward()\n",
    "    ```\n",
    "    Gradients are stored in `x.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient-Free Computations**\n",
    "Sometimes, gradients are unnecessary (e.g., during evaluation). Use:\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # Perform operations without tracking gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PyTorch Dataloader**\n",
    "The **Dataloader** handles large datasets by splitting them into manageable batches for training.  \n",
    "- **Key Features:**\n",
    "  - Efficient memory management.\n",
    "  - Shuffling data for better generalization.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for batch in loader:\n",
    "    # Process each batch\n",
    "```\n",
    "- **Random Split:** Splits datasets into training and validation sets:\n",
    "  ```python\n",
    "  train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Networks with `torch.nn`**\n",
    "- **Building Blocks:** Neural networks are built using the `torch.nn` package.\n",
    "- **Core Component: `nn.Module`:**\n",
    "  - Contains layers like `nn.Linear`, `nn.Conv2d`, etc.\n",
    "  - Methods include `forward()` for defining the forward pass.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "- **Training Algorithm:**\n",
    "  1. Input data.\n",
    "  2. Compute predictions.\n",
    "  3. Calculate loss (e.g., `nn.MSELoss` or `nn.BCELoss`).\n",
    "  4. Perform backpropagation using `.backward()`.\n",
    "  5. Update weights using an optimizer like `SGD` or `Adam`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Normalization and Transforms**\n",
    "1. **Normalization:** Ensures input data is scaled for efficient training.\n",
    "   Example using `torchvision.transforms`:\n",
    "   ```python\n",
    "   transforms.Normalize(mean, std)\n",
    "   ```\n",
    "\n",
    "2. **Rearranging Dimensions:**  \n",
    "   Use `numpy.transpose()` to change the shape of data:\n",
    "   ```python\n",
    "   data = np.transpose(data, (2, 0, 1))  # Channels-first format\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Labels and Classes**\n",
    "- In datasets, the **labels** represent the **class** (category) to which each data sample belongs.\n",
    "\n",
    "Example:\n",
    "- A dataset for image classification might have labels like:\n",
    "  - `0`: Cat\n",
    "  - `1`: Dog\n",
    "  - `2`: Bird\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
