{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __INTRODUCTION__ \n",
    "###### _Today's Class was By From Haribabu Sir_:\n",
    "We go through ML in three approaches mathematics, programming, and intuition.\n",
    "\n",
    "Artificial Intelligence: We, humans are trying to impart knowledge or intelligence to the machines to make changes in the day to day world. Mimicking human intelligence in humans.\n",
    "\n",
    "Evolutionary organisms, Machine learning. Humans basically learn from experiences but machines follow the instruction given by us. This is Traditional Programming. But we now want humans to learn from experience too. More metaphorically learn from data. Which is Training the machine for machine learning.\n",
    "\n",
    "In ML terminalogy equations which are found are called to be model. Hyperplanes are imaginary multidimensional planes which are built by computers. Predicting a definite value is known as Regression in ML  and Predicting the category of class or range is known as classification.\n",
    "\n",
    "\n",
    "In ML we already have the 'Ground Truth' Or Target (y , Label). 2 categories- binary classification else Multi class classification. Why do we need DL when ML is already there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Neural Networks:__\n",
    "###### _A Game Changer Perceptron, Forward Propagation_\n",
    "\n",
    "When we combine number of Gates we get transistors. Neural Network is highly inspired by the Biological Neuron. Transformers are based on encoder decoder architectures. Feed forward Networks are also essential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return 2*x+1\n",
    "print(func(int(input())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the same with an array.\n",
    "\n",
    "_A 2 dimensional array multiplied by a scalar quantity and then added by one._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 2 2]\n",
      " [2 1 2]]\n",
      "[[3 3 3]\n",
      " [3 5 5]\n",
      " [5 3 5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "a=np.random.randint(1,3,(3,3))\n",
    "def func1(a):\n",
    "    print(a)\n",
    "    return 2*a+1\n",
    "print(func1(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now moving forward towards matrix multiplication..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 2]\n",
      " [1 1 2]\n",
      " [1 2 2]]\n"
     ]
    }
   ],
   "source": [
    "c=np.random.randint(1,3,(3,3))\n",
    "d=np.random.randint(1,4,(3,3))\n",
    "def multiplicationofmatrices(x,y):\n",
    "    y=x*y\n",
    "    return y\n",
    "print(multiplicationofmatrices(c,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now Implementing the above using TENSORS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 2, 1],\n",
      "        [4, 4, 1],\n",
      "        [4, 1, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "e=torch.randint(1,3,(3,3))\n",
    "f=torch.randint(1,3,(3,3))\n",
    "def multiplicationoftensor(x,y):\n",
    "    y=x*y\n",
    "    return y\n",
    "print(multiplicationofmatrices(e,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, `tensor.random` has no attribute such as `randint`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Steps in Deep Learning:__ (_Starting with Simple Perceptron_):\n",
    "__Step 1: Data Preparation__: _Processing the Data, Preprocessing, Visualizing the Data, Having the Truth Value._\n",
    "\n",
    "__Step 2: Model Design__: _Neural Network Architecture, Actualize Parameters._\n",
    "\n",
    "__Step 3: Forward Propagation__: _Feeding the input to the model. We get the predicted value here._\n",
    "\n",
    "__Step 4: Loss Calculation__: _Difference between Predicted Value and Truth Value. (ex. Mean Square Error, Binary Cross Entropy)_\n",
    "\n",
    "__Step 5: Optimazation__: _We try to reduce the loss _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __FEW POINTS__:\n",
    "###### _That I thought are important and Intresting_\n",
    "MAE,MSE Loss Calculation functions are most suitable for Regression. \n",
    "__MAE vs MSE in terms of Outliers__:\n",
    "\n",
    "__MAE (Mean Absolute Error)__: Less sensitive to outliers because it calculates the absolute difference, avoiding squared amplification.\n",
    "\n",
    "__MSE (Mean Squared Error)__: Highly sensitive to outliers since squaring large errors makes them dominate the loss.\n",
    "\n",
    "We need to get or choose the best values of w and b so that they could be precise to the Truth Values. We should be able to or atleast try to increase 'w' and decrease 'b' for better pattern learning and reduced dependency on constant shifts in our Neural Network.\n",
    "\n",
    "We use different gradients in this case for the redution of Loss. Which is really very similar to the Derivative Formula.\n",
    "So we have to move near to the Optima in both the cases such as Positive Slope and Negative Slope. This is known as convergence. \n",
    "\n",
    "We have to be far from the saddle points in this case since gradients in saddle points is zero which means zero progress in finding the loss. The learning rate ($\\alpha$) controls the step size of updates in gradient descent. Which if is bigger than shall oscillate more and if is smaller then shall be time taking so it must be balanced. \n",
    "Use MAE for robust models and MSE when penalizing large errors heavily is desired.\n",
    "\n",
    "__Note__: An __Outlier__ is a data point that significantly deviates from the rest of the dataset. It may indicate variability, errors, or anomalies.\n",
    "_Example_: A house priced at $10M in a dataset where most houses cost $100Kâ€“$500K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Simple Gradient Descent Algorithm:__\n",
    "\n",
    "```python\n",
    "t=0\n",
    "epochs=3\n",
    "while(t<=epochs):\n",
    "    w=w-learningRate*dw\n",
    "    b=b-learningRate*db\n",
    "    t+=1\n",
    "``` \n",
    "\n",
    "This is a **gradient descent algorithm** for updating weights ($w$) and bias ($b$) over a specified number of iterations (epochs).\n",
    "\n",
    "### Explanation:\n",
    "1. **Initialization**:  \n",
    "   Start with $t = 0$ (iteration counter) and a defined number of epochs.\n",
    "\n",
    "2. **Update Rules**:  \n",
    "   At each iteration, the weights and bias are updated using:  \n",
    "   $$ w = w - \\alpha \\cdot \\frac{\\partial J}{\\partial w} $$  \n",
    "   $$ b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b} $$  \n",
    "   where:\n",
    "   - $\\alpha$ is the learning rate.\n",
    "   - $\\frac{\\partial J}{\\partial w}$ and $\\frac{\\partial J}{\\partial b}$ are gradients of the cost function $J$ w.r.t. $w$ and $b$.\n",
    "\n",
    "3. **Iteration**:  \n",
    "   The updates are repeated for `epochs` times, incrementing $t$ after each update.\n",
    "\n",
    "### Purpose:  \n",
    "The algorithm minimizes the cost function $J(w, b)$ by iteratively adjusting $w$ and $b$ to reduce the error between predictions and actual values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
