{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __EVALUATION METRICS__:\n",
    "### **1. What Is a Confusion Matrix?**\n",
    "It is a matrix of size $n \\times n$, where $n$ is the number of classes in the classification task. For a binary classification problem, the confusion matrix looks like this:\n",
    "\n",
    "| **Actual \\ Predicted** | **Positive** | **Negative** |\n",
    "|-|--|--|\n",
    "| **Positive**            | **True Positive (TP)** | **False Negative (FN)** |\n",
    "| **Negative**            | **False Positive (FP)** | **True Negative (TN)** |\n",
    "\n",
    "\n",
    "### **2. Terminology in Binary Classification**\n",
    "\n",
    "#### **True Positive (TP):**\n",
    "- Correctly predicted **positive** instances.  \n",
    "  Example: A spam email correctly identified as spam.\n",
    "\n",
    "#### **True Negative (TN):**\n",
    "- Correctly predicted **negative** instances.  \n",
    "  Example: A non-spam email correctly identified as non-spam.\n",
    "\n",
    "#### **False Positive (FP):**\n",
    "- Incorrectly predicted **positive** instances (also called a **Type I Error**).  \n",
    "  Example: A non-spam email incorrectly identified as spam.\n",
    "\n",
    "#### **False Negative (FN):**\n",
    "- Incorrectly predicted **negative** instances (also called a **Type II Error**).  \n",
    "  Example: A spam email incorrectly identified as non-spam.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Metrics Derived From the Confusion Matrix**\n",
    "The confusion matrix allows the calculation of several performance metrics:\n",
    "\n",
    "#### **Accuracy:**\n",
    "- Proportion of correctly predicted instances:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "#### **Precision (Positive Predictive Value):**\n",
    "- Proportion of correctly predicted positives out of all predicted positives:\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "#### **Recall (Sensitivity or True Positive Rate):**\n",
    "- Proportion of correctly predicted positives out of all actual positives:\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "#### **F1-Score:**\n",
    "- Harmonic mean of Precision and Recall, balancing both metrics:\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "#### **Specificity (True Negative Rate):**\n",
    "- Proportion of correctly predicted negatives out of all actual negatives:\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "\n",
    "### **4. Why Use a Confusion Matrix?**\n",
    "\n",
    "1. **Detailed Analysis:** Unlike accuracy, which provides a single value, the confusion matrix breaks down results for each class.\n",
    "2. **Imbalanced Data:** For datasets with uneven class distribution, metrics like Precision and Recall from the confusion matrix provide better insight.\n",
    "3. **Multi-Class Classification:** For $n$ classes, the confusion matrix shows performance for each class.\n",
    "\n",
    "\n",
    "### **5. Example of a Confusion Matrix**\n",
    "#### Scenario:\n",
    "- Task: Binary classification to detect spam emails.\n",
    "- Dataset: 100 emails (80 spam, 20 non-spam).\n",
    "- Results:\n",
    "  - 70 spam emails correctly identified (TP).\n",
    "  - 10 spam emails misclassified as non-spam (FN).\n",
    "  - 15 non-spam emails incorrectly identified as spam (FP).\n",
    "  - 5 non-spam emails correctly identified (TN).\n",
    "\n",
    "#### Confusion Matrix:\n",
    "| **Actual \\ Predicted** | **Spam** | **Non-Spam** |\n",
    "|-|-|--|\n",
    "| **Spam**               | 70       | 10           |\n",
    "| **Non-Spam**           | 15       | 5            |\n",
    "\n",
    "#### Metrics:\n",
    "- Accuracy:\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{70 + 5}{70 + 10 + 15 + 5} = 75\\%\n",
    "  $$\n",
    "- Precision:\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{70}{70 + 15} = 82.35\\%\n",
    "  $$\n",
    "- Recall:\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{70}{70 + 10} = 87.5\\%\n",
    "  $$\n",
    "- F1-Score:\n",
    "  $$\n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{0.8235 \\cdot 0.875}{0.8235 + 0.875} \\approx 85\\%\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1-score** uses the **harmonic mean** of Precision and Recall instead of the arithmetic mean because the harmonic mean emphasizes balance between the two metrics, ensuring that low values in either Precision or Recall have a significant impact on the score.\n",
    "\n",
    "\n",
    "### **Why Harmonic Mean for F1-Score?**\n",
    "\n",
    "1. **Balancing Precision and Recall:**\n",
    "   - The F1-score ensures a **trade-off** between Precision and Recall. \n",
    "   - If either Precision or Recall is low, the F1-score will be heavily penalized.\n",
    "   - For example, if Precision = 1 and Recall = 0.1, the arithmetic mean is:\n",
    "     $$\n",
    "     \\text{Arithmetic Mean} = \\frac{1 + 0.1}{2} = 0.55\n",
    "     $$\n",
    "     The harmonic mean, however, is:\n",
    "     $$\n",
    "     \\text{Harmonic Mean (F1)} = 2 \\cdot \\frac{1 \\cdot 0.1}{1 + 0.1} = 0.18\n",
    "     $$\n",
    "     This better reflects the poor Recall.\n",
    "\n",
    "2. **Sensitivity to Extremes:**\n",
    "   - The harmonic mean is more sensitive to **low values** than the arithmetic mean. This makes it ideal for scenarios where **both Precision and Recall are critical** (e.g., in medical diagnosis or fraud detection).\n",
    "\n",
    "3. **Geometric Interpretation:**\n",
    "   - The harmonic mean considers the **inverse of the averages**, ensuring that both Precision and Recall contribute equally to the F1-score.\n",
    "\n",
    "### **Formula Recap**\n",
    "The F1-score is given by:\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "Where:\n",
    "- **Precision** measures how many predicted positives are actually correct:\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "- **Recall** measures how many actual positives are correctly predicted:\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "\n",
    "### **Why Is F1-Score Important?**\n",
    "1. **Imbalanced Datasets:**\n",
    "   - In datasets with imbalanced classes, accuracy can be misleading.\n",
    "   - The F1-score ensures that both Precision and Recall are considered equally.\n",
    "   - Example: A classifier predicting all negatives in a rare event dataset may achieve high accuracy but fail in Recall, which the F1-score penalizes.\n",
    "\n",
    "2. **Applications:**\n",
    "   - **Medical Diagnosis:** Where both false positives and false negatives carry risks.\n",
    "   - **Spam Detection:** Avoiding spam requires high Precision, but catching most spam requires high Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **classification report** in machine learning provides a detailed summary of the performance of a classification model. It includes metrics like Precision, Recall, and F1-score for each class, as well as overall averages like **macro average** and **weighted average**.\n",
    "\n",
    "### **1. What is a Classification Report?**\n",
    "\n",
    "A **classification report** breaks down the model's performance across individual classes, showing how well it predicts each one. It typically includes:\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-Score**\n",
    "- **Support** (number of true instances for each class)\n",
    "\n",
    "#### Example Format (For a 3-Class Problem):\n",
    "| Class       | Precision | Recall | F1-Score | Support |\n",
    "|-|--|--|-||\n",
    "| Class 0     | 0.90      | 0.85   | 0.87     | 50      |\n",
    "| Class 1     | 0.80      | 0.75   | 0.77     | 30      |\n",
    "| Class 2     | 0.70      | 0.65   | 0.67     | 20      |\n",
    "| **Macro Avg** | 0.80      | 0.75   | 0.77     | 100     |\n",
    "| **Weighted Avg** | 0.83      | 0.80   | 0.81     | 100     |\n",
    "\n",
    "### **2. Key Metrics in a Classification Report**\n",
    "\n",
    "#### **Precision**  \n",
    "Proportion of correctly predicted positives out of all predicted positives:\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "#### **Recall (Sensitivity)**  \n",
    "Proportion of correctly predicted positives out of all actual positives:\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "#### **F1-Score**  \n",
    "Harmonic mean of Precision and Recall:\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "#### **Support**  \n",
    "The number of true instances for each class in the dataset.  \n",
    "- Useful to understand the distribution of data across classes.\n",
    "\n",
    "### **3. Macro Average vs. Weighted Average**\n",
    "\n",
    "#### **Macro Average**\n",
    "- **Definition:** Calculates metrics for each class independently and takes the unweighted mean.  \n",
    "$$\n",
    "\\text{Macro Average} = \\frac{\\text{Metric(Class 0)} + \\text{Metric(Class 1)} + \\ldots}{\\text{Number of Classes}}\n",
    "$$\n",
    "\n",
    "- **Purpose:**  \n",
    "  - Gives equal importance to all classes, regardless of their frequency.\n",
    "  - Useful when class distribution is balanced.\n",
    "\n",
    "- **Drawback:**  \n",
    "  - If some classes have very few instances, macro averages might overemphasize their metrics.\n",
    "\n",
    "#### **Weighted Average**\n",
    "- **Definition:** Calculates metrics for each class, weighted by the class's support (number of instances).  \n",
    "$$\n",
    "\\text{Weighted Avg Metric} = \\frac{\\sum (\\text{Metric(Class)} \\times \\text{Support(Class)})}{\\text{Total Instances}}\n",
    "$$\n",
    "\n",
    "- **Purpose:**  \n",
    "  - Accounts for class imbalance.\n",
    "  - Dominant classes contribute more to the average.\n",
    "\n",
    "- **Example:**\n",
    "  If Class 0 has 80 instances and Class 1 has 20, Class 0 contributes 80% to the weighted average, while Class 1 contributes 20%.\n",
    "\n",
    "### **4. Micro Average**\n",
    "- **Definition:** Combines all True Positives, False Positives, and False Negatives across classes and calculates metrics globally.  \n",
    "$$\n",
    "\\text{Micro Precision} = \\frac{\\text{Total TP}}{\\text{Total TP + Total FP}}\n",
    "$$\n",
    "\n",
    "- **Purpose:**  \n",
    "  - Useful for evaluating multi-class problems with imbalanced data.  \n",
    "  - Treats all instances equally, regardless of class.\n",
    "\n",
    "### **5. When to Use Each Average?**\n",
    "\n",
    "| **Metric**        | **Use Case**                                                                 |\n",
    "|--|--|\n",
    "| **Macro Average**  | When all classes are equally important, regardless of their size.           |\n",
    "| **Weighted Average** | When class imbalance exists and larger classes are more significant.      |\n",
    "| **Micro Average**  | For multi-class problems where the overall performance is the focus.        |\n",
    "### **6. Python Example: Generating a Classification Report**\n",
    "\n",
    "#### **Code Example:**\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example true and predicted labels\n",
    "y_true = [0, 1, 2, 0, 1, 2, 1, 0, 2, 1]\n",
    "y_pred = [0, 1, 2, 0, 0, 2, 1, 0, 1, 2]\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Class 0\", \"Class 1\", \"Class 2\"])\n",
    "print(report)\n",
    "```\n",
    "\n",
    "#### **Output:**\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    Class 0       1.00      1.00      1.00         3\n",
    "    Class 1       0.50      0.67      0.57         4\n",
    "    Class 2       0.67      0.50      0.57         3\n",
    "\n",
    "    accuracy                           0.70        10\n",
    "   macro avg       0.72      0.72      0.71        10\n",
    "weighted avg       0.72      0.70      0.70        10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **regression tasks**, the objective is to predict continuous values, and the performance of the model is evaluated using various metrics. Each metric assesses different aspects of the model's predictive accuracy and error.\n",
    "\n",
    "### **1. Common Regression Metrics**\n",
    "\n",
    "#### **1.1. Mean Absolute Error (MAE):**\n",
    "- Measures the average absolute difference between predicted values ($\\hat{y}$) and true values ($y$).\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n \\lvert \\hat{y}_i - y_i \\rvert\n",
    "$$\n",
    "\n",
    "- **Characteristics:**\n",
    "  - Easy to interpret: It shows the average error in the same units as the target variable.\n",
    "  - Less sensitive to outliers than other metrics.\n",
    "\n",
    "- **Use Case:** When you want a simple and robust measure of average prediction error.\n",
    "\n",
    "#### **1.2. Mean Squared Error (MSE):**\n",
    "- Measures the average squared difference between predicted and actual values.\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "- **Characteristics:**\n",
    "  - Penalizes large errors more than small ones because of squaring.\n",
    "  - Sensitive to outliers.\n",
    "\n",
    "- **Use Case:** When large errors are undesirable and should be heavily penalized (e.g., financial forecasting).\n",
    "\n",
    "#### **1.3. Root Mean Squared Error (RMSE):**\n",
    "- The square root of MSE, providing error in the same units as the target variable.\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "- **Characteristics:**\n",
    "  - Easier to interpret than MSE because it’s in the same scale as the target variable.\n",
    "  - Sensitive to outliers.\n",
    "\n",
    "- **Use Case:** Similar to MSE but more interpretable in terms of scale.\n",
    "\n",
    "#### **1.4. Mean Absolute Percentage Error (MAPE):**\n",
    "- Expresses error as a percentage of the true values.\n",
    "$$\n",
    "\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^n \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\n",
    "$$\n",
    "\n",
    "- **Characteristics:**\n",
    "  - Useful when the magnitude of the target variable varies widely.\n",
    "  - Can be misleading if $y_i \\approx 0$ because percentages can explode.\n",
    "\n",
    "- **Use Case:** When relative errors matter (e.g., sales forecasting).\n",
    "\n",
    "#### **1.5. R-Squared ($R^2$ or Coefficient of Determination):**\n",
    "- Measures how well the model explains the variance in the target variable.\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- **Interpretation:**\n",
    "  - $R^2 = 1$: Perfect fit.\n",
    "  - $R^2 = 0$: Model performs as poorly as predicting the mean $\\bar{y}$.\n",
    "  - $R^2 < 0$: Model is worse than predicting $\\bar{y}$.\n",
    "\n",
    "- **Use Case:** To assess the proportion of variance explained by the model.\n",
    "\n",
    "#### **1.6. Adjusted R-Squared:**\n",
    "- Adjusts $R^2$ for the number of predictors in the model:\n",
    "$$\n",
    "R^2_{\\text{adjusted}} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\n",
    "$$\n",
    "Where:\n",
    "- $n$: Number of observations.\n",
    "- $p$: Number of predictors.\n",
    "\n",
    "- **Use Case:** When comparing models with different numbers of predictors.\n",
    "\n",
    "### **2. Choosing the Right Metric**\n",
    "\n",
    "| **Metric** | **When to Use** |\n",
    "||--|\n",
    "| **MAE**    | When simplicity and interpretability are important, and outliers are less critical. |\n",
    "| **MSE**    | When penalizing large errors is essential. |\n",
    "| **RMSE**   | When you need interpretable error in the same units as the target variable. |\n",
    "| **MAPE**   | When relative error is more meaningful than absolute error. |\n",
    "| **R-Squared** | To measure how much variance the model explains. |\n",
    "| **Adjusted R-Squared** | To compare models with different numbers of predictors. |\n",
    "\n",
    "### **3. Python Implementation**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "y_true = [100, 200, 300, 400, 500]\n",
    "y_pred = [110, 190, 310, 420, 490]\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# MSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# R-Squared\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-Squared: {r2}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **$R^2$ (Coefficient of Determination)** metric is widely used to evaluate how well a regression model explains the variability of the target variable. However, it has some significant **drawbacks** that limit its effectiveness in certain scenarios.\n",
    "\n",
    "### **1. $R^2$ Does Not Indicate Model Accuracy**\n",
    "- **Why:** A high $R^2$ does not guarantee that the model's predictions are accurate.\n",
    "- **Example:**\n",
    "  - If predictions systematically deviate from actual values (e.g., consistently overestimating), $R^2$ can still be high as long as the predictions follow the general trend.\n",
    "\n",
    "- **Key Insight:** $R^2$ measures how well the model fits the data's variability but does not indicate how close predictions are to actual values.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Sensitive to Outliers**\n",
    "- **Why:** Outliers can inflate or deflate $R^2$ because it is based on squared differences.\n",
    "- **Example:**\n",
    "  - A single extreme value can cause $R^2$ to increase, even if the model performs poorly on the rest of the data.\n",
    "  \n",
    "- **Key Insight:** $R^2$ does not robustly handle datasets with outliers.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Biased Toward Complex Models**\n",
    "- **Why:** Adding more predictors to a model, even irrelevant ones, can increase $R^2$, as it always increases or remains constant when additional predictors are included.\n",
    "\n",
    "- **Key Insight:** This overestimation can give the illusion of a better model, even if the added predictors do not improve performance.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Cannot Evaluate Non-Linear Relationships**\n",
    "- **Why:** $R^2$ assumes that the relationship between predictors and the target variable is linear.\n",
    "- **Example:**\n",
    "  - In a non-linear problem, $R^2$ may give a poor score even if the model makes accurate predictions because it doesn't capture the curvature in the data.\n",
    "\n",
    "- **Key Insight:** $R^2$ is not suitable for non-linear regression models without modifications.\n",
    "\n",
    "\n",
    "\n",
    "### **5. Does Not Reflect Practical Significance**\n",
    "- **Why:** $R^2$ is a statistical measure and does not necessarily align with the real-world importance of a model's predictions.\n",
    "\n",
    "- **Example:**\n",
    "  - A model with $R^2 = 0.80$ may still be unacceptable in medical diagnostics if its predictions lead to critical errors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **6. Negative $R^2$ Values**\n",
    "- **Why:** $R^2$ can be negative when the model performs worse than predicting the mean $\\bar{y}$.\n",
    "- **Example:**\n",
    "  - A poorly fitted model with random predictions might have $R^2 < 0$, indicating that even using the mean as a prediction would be better.\n",
    "  \n",
    "- **Key Insight:** Negative $R^2$ values highlight a lack of explanatory power but are often misunderstood.\n",
    "\n",
    "\n",
    "\n",
    "### **7. Does Not Handle Class Imbalances (For Classification Tasks)**\n",
    "- Although $R^2$ is primarily used in regression, when mistakenly applied to classification-like tasks with numeric targets, it fails to handle imbalances in target variable distributions.\n",
    "\n",
    "### **Alternative Metrics to Overcome $R^2$ Drawbacks**\n",
    "1. **Adjusted $R^2$:**\n",
    "   - Penalizes models for adding irrelevant predictors.\n",
    "   - Formula:\n",
    "     $$\n",
    "     R^2_{\\text{adjusted}} = 1 - \\frac{(1 - R^2)(n - 1)}{n - p - 1}\n",
    "     $$\n",
    "     Where:\n",
    "     - $n$: Number of observations.\n",
    "     - $p$: Number of predictors.\n",
    "\n",
    "2. **Mean Absolute Error (MAE):**\n",
    "   - Measures average absolute prediction error.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE):**\n",
    "   - Provides error in the same units as the target variable.\n",
    "\n",
    "4. **Mean Absolute Percentage Error (MAPE):**\n",
    "   - Expresses error as a percentage of actual values, offering interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard Scaling** is a preprocessing technique used in machine learning to transform features so that they have a **mean of 0** and a **standard deviation of 1**. This ensures that all features are on the same scale, which is crucial for many machine learning algorithms to perform optimally.\n",
    "\n",
    "### **1. Formula for Standard Scaling**\n",
    "Each feature in the dataset is scaled using the formula:\n",
    "$$\n",
    "x' = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "Where:\n",
    "- \\(x\\): Original feature value.\n",
    "- \\(\\mu\\): Mean of the feature.\n",
    "- \\(\\sigma\\): Standard deviation of the feature.\n",
    "- \\(x'\\): Scaled feature value.\n",
    "\n",
    "### **2. Why Use Standard Scaling?**\n",
    "\n",
    "#### **2.1. Avoiding Feature Dominance**\n",
    "- Features with large ranges (e.g., income in thousands vs. age in years) can dominate distance-based models like **K-Nearest Neighbors (KNN)**, **SVM**, and **Logistic Regression**.\n",
    "- Standard scaling ensures all features contribute equally.\n",
    "\n",
    "#### **2.2. Improving Gradient Descent**\n",
    "- For optimization algorithms (e.g., gradient descent), scaling helps the algorithm converge faster by maintaining balanced updates across all weights.\n",
    "\n",
    "#### **2.3. Required for Models Sensitive to Feature Magnitudes**\n",
    "Standard scaling is particularly useful for:\n",
    "- **Distance-based models:** KNN, SVM, clustering algorithms.\n",
    "- **Linear models:** Logistic Regression, Linear Regression.\n",
    "- **Principal Component Analysis (PCA):** PCA projects data onto orthogonal components, and unscaled features can skew the results.\n",
    "\n",
    "### **3. How Standard Scaling Works**\n",
    "#### Example Dataset:\n",
    "| Feature 1 | Feature 2 |\n",
    "|-----------|-----------|\n",
    "| 10        | 100       |\n",
    "| 20        | 200       |\n",
    "| 30        | 300       |\n",
    "\n",
    "1. Compute the **mean (\\(\\mu\\))** and **standard deviation (\\(\\sigma\\))** for each feature:\n",
    "   - Feature 1: \\(\\mu = 20, \\sigma = 10\\)\n",
    "   - Feature 2: \\(\\mu = 200, \\sigma = 100\\)\n",
    "\n",
    "2. Apply the standard scaling formula to each value:\n",
    "   - For \\(x = 10\\) in Feature 1:\n",
    "     $$\n",
    "     x' = \\frac{10 - 20}{10} = -1\n",
    "     $$\n",
    "   - For \\(x = 300\\) in Feature 2:\n",
    "     $$\n",
    "     x' = \\frac{300 - 200}{100} = 1\n",
    "     $$\n",
    "\n",
    "3. Scaled Dataset:\n",
    "| Feature 1 | Feature 2 |\n",
    "|-----------|-----------|\n",
    "| -1        | -1        |\n",
    "| 0         | 0         |\n",
    "| 1         | 1         |\n",
    "\n",
    "### **4. Standard Scaling vs. Other Scaling Methods**\n",
    "| **Scaling Method**       | **Description**                                                                                   | **When to Use**                                                                          |\n",
    "|--------------------------|---------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
    "| **Standard Scaling**      | Scales data to mean 0, standard deviation 1.                                                     | Models sensitive to magnitude (SVM, Logistic Regression, KNN, PCA).                    |\n",
    "| **Min-Max Scaling**       | Scales data to range \\([0, 1]\\) or \\([-1, 1]\\).                                                  | Neural networks or when absolute feature bounds are required.                          |\n",
    "| **Robust Scaling**        | Uses median and interquartile range (IQR) to scale data, making it robust to outliers.           | Data with significant outliers.                                                        |\n",
    "\n",
    "### **5. Python Implementation**\n",
    "\n",
    "#### **Using Scikit-learn:**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.array([[10, 100], [20, 200], [30, 300]])\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data:\\n\", scaled_data)\n",
    "```\n",
    "\n",
    "#### **Output:**\n",
    "```\n",
    "Original Data:\n",
    " [[ 10 100]\n",
    " [ 20 200]\n",
    " [ 30 300]]\n",
    "Scaled Data:\n",
    " [[-1. -1.]\n",
    "  [ 0.  0.]\n",
    "  [ 1.  1.]]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
